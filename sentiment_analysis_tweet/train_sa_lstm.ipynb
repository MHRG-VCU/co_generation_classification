{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from twodlearn.tf_lib.Feedforward import LinearLayer\n",
    "\n",
    "import sys\n",
    "working_dir= '/home/marinodl/dl_projects/nlp/sentiment_analysis/'\n",
    "sys.path.insert(0, working_dir)\n",
    "\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Allow_Valid= True\n",
    "Valid_Percentage= 0.1 # i.e. 10% of the data will be used for validation\n",
    "\n",
    "pos_net_name= '_pos'\n",
    "neg_net_name= '_neg'\n",
    "\n",
    "activation_function='tanh'\n",
    "num_nodes = [500,500,500] #64 500, num_nodes: Nodes for the LSTM cell\n",
    "alpha = 0.01 #10.1 #0.1\n",
    "beta = 0.001 #10.1 #0.01\n",
    "lambda_w = 0.00001\n",
    "\n",
    "dropout_cons = 1.0\n",
    "\n",
    "Allow_Bias= False \n",
    "\n",
    "learning_rate= 0.0001      # 0.001\n",
    "grad_clip_thresh= 0.1       # 0.00001\n",
    "\n",
    "current_run= 1\n",
    "batch_size= 64 #64\n",
    "num_unrollings= 100 #100\n",
    "\n",
    "batch_size_val= 64 #len(valid_text_pos)/num_unrollings_val #500\n",
    "\n",
    "comment='_01alpha_noDropout_LcLpLcp_100unrol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: [500, 500, 500]\n",
      "alpha: 0.01 , beta: 0.001 , lambda_w: 1e-05\n",
      "num_unrollings: 100 , batch_size: 64 , batch_size_val: 64\n"
     ]
    }
   ],
   "source": [
    "model_version = 'L'+str(len(num_nodes))\n",
    "print(\"num_nodes:\",num_nodes)\n",
    "print(\"alpha:\",alpha, \", beta:\",beta,\", lambda_w:\", lambda_w)\n",
    "print(\"num_unrollings:\",num_unrollings, \", batch_size:\",batch_size,\", batch_size_val:\", batch_size_val)\n",
    "print(\"learning_rate:\", learning_rate, 'grad_clip_thresh:', grad_clip_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7985367\n"
     ]
    }
   ],
   "source": [
    "#with open('/home/marinodl/datasets/nlp/without_html/positive.txt', 'r') as file_handler:\n",
    "with open('/home/marinodl/datasets/nlp/imdb/train/imdb_positive.txt', 'r', errors='ignore') as file_handler:\n",
    "    text_pos = file_handler.read()\n",
    "    \n",
    "#with open('/home/marinodl/datasets/nlp/without_html/negative.txt', 'r') as file_handler:\n",
    "with open('/home/marinodl/datasets/nlp/imdb/train/imdb_negative.txt', 'r', errors='ignore') as file_handler:\n",
    "    text_neg = file_handler.read()\n",
    "\n",
    "len_text= min(len(text_pos), len(text_neg))//2 #CAREFUL\n",
    "\n",
    "print(len_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "798536\n",
      "7186831\n"
     ]
    }
   ],
   "source": [
    "valid_size_pos = int(math.floor(len_text)*0.1)\n",
    "valid_text_pos = text_pos[:valid_size_pos]\n",
    "train_text_pos = text_pos[valid_size_pos:len_text] # excluding validation set\n",
    "#train_text_pos = text_pos[:len_text]               # including validation set\n",
    "train_size_pos = len(train_text_pos)\n",
    "\n",
    "print(valid_size_pos)\n",
    "print(train_size_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7186831\n"
     ]
    }
   ],
   "source": [
    "valid_size_neg = int(math.floor(len_text*0.1))\n",
    "valid_text_neg = text_neg[:valid_size_neg]\n",
    "train_text_neg = text_neg[valid_size_neg:len_text] # excluding validation set\n",
    "#train_text_neg = text_neg[:len_text]               # including validation set\n",
    "train_size_neg = len(train_text_neg)               \n",
    "print(train_size_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vocabulary_coding_simple import *\n",
    "\n",
    "vc= Vocabulary('')\n",
    "num_inputs=  vc.vocabulary_size\n",
    "num_outputs= vc.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 0, 26, 0, 0, 0]\n",
      "['a', 'b', 'c', ' ', 'z', ' ', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "print([vc.char2id(i) for i in 'abc z.[]'])\n",
    "print([vc.id2char(i) for i in [1, 2, 3, 0, 26, 27, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vc.vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, vc.char2id(self._text[self._cursor[b]])] = 1.0    # the letter pointed by the cursor is converted to 1-hot encoding\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size  # Here, the cursor is increased\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch] # include last batch from previous array\n",
    "        #batches = list()\n",
    "        for step in range(self._num_unrollings):\n",
    "            # each call of _next_batch() increases the cursor by one\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10 # this is to prevent that log() returns minus infinity\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vc.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vc.vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get supervised-batch from positive and negative texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches_pos = BatchGenerator(train_text_pos, batch_size//2, num_unrollings)\n",
    "batches_neg = BatchGenerator(train_text_neg, batch_size//2, num_unrollings)\n",
    "\n",
    "def GetNextBatch_sentiment(rand_shuffle= False):\n",
    "    ''' Generates a batch of sentences and its corresponding labels for classification\n",
    "        Returns:\n",
    "            X: list of positive and negative sentences, the format is the same that the batches generated from the \n",
    "               positive and negative texts. len(X)= num_unrrolings, X[i].shape: [batch_size x vocabulary_size]\n",
    "            y: class to which each sentence belong. The format is a vector of size [batch_size x 1]\n",
    "    '''\n",
    "    batch_pos = batches_pos.next()\n",
    "    batch_neg = batches_neg.next()\n",
    "        \n",
    "    y_aux= np.zeros(shape=[batch_size,1], dtype=np.float) # y_aux= class for each one of the sentences\n",
    "    y_aux[0:int(batch_size//2),0]= 1 # positive samples are denoted by 1, negative by 0\n",
    "    \n",
    "    # TODO: ADD MULTIPLE CLASSES\n",
    "    if rand_shuffle:\n",
    "        rand_ind= np.random.permutation(batch_size) # rand_ind= random shuffling for the sentences\n",
    "        y_aux=y_aux[rand_ind,:]\n",
    "    \n",
    "    X= list()\n",
    "    #y= list()\n",
    "    # TODO: try to do not penalize mistakes being done in the first samples of the unrolling\n",
    "    # start_ind= math.floor(0.5*len(batch_pos))\n",
    "    for x_ind in range(len(batch_pos)):  # the length of batch_pos is the number of unrollings\n",
    "        # Concatenate positive and negative batches and arrange them according to rand_ind\n",
    "        if rand_shuffle:\n",
    "            X.append( np.concatenate((batch_pos[x_ind],batch_neg[x_ind]),0)[rand_ind,:] )\n",
    "        else:\n",
    "            X.append( np.concatenate((batch_pos[x_ind],batch_neg[x_ind]),0) )\n",
    "        \n",
    "    return X, y_aux\n",
    "\n",
    "batch_X, batch_y = GetNextBatch_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_val: 64  suggested: 7985.36\n"
     ]
    }
   ],
   "source": [
    "num_unrollings_val= 100 #num_unrollings #10 #num_unrollings \n",
    "print('batch_size_val:', batch_size_val, ' suggested:', len(valid_text_pos)/num_unrollings_val)\n",
    "\n",
    "batches_pos_val = BatchGenerator(valid_text_pos, batch_size_val//2, num_unrollings_val)\n",
    "batches_neg_val = BatchGenerator(valid_text_neg, batch_size_val//2, num_unrollings_val)\n",
    "#batches_pos_val = BatchGenerator(train_text_pos, batch_size_val/2, num_unrollings_val)\n",
    "#batches_neg_val = BatchGenerator(train_text_neg, batch_size_val/2, num_unrollings_val)\n",
    "\n",
    "\n",
    "def Get_Val():\n",
    "    batch_pos = batches_pos_val.next()\n",
    "    batch_neg = batches_neg_val.next()\n",
    "        \n",
    "    y_aux= np.zeros(shape=[batch_size_val,1], dtype=np.float) # y_aux= class for each one of the sentences\n",
    "    y_aux[0:int(batch_size_val//2),0]= 1\n",
    "    \n",
    "    # TODO: ADD MULTIPLE CLASSES\n",
    "    rand_ind= np.random.permutation(batch_size_val) # rand_ind= random shuffling for the sentences\n",
    "    \n",
    "    y_aux=y_aux[rand_ind,:]\n",
    "    \n",
    "    X= list()\n",
    "    #y= list()\n",
    "    # TODO: try to do not penalize mistakes being done in the first samples of the unrolling\n",
    "    # start_ind= math.floor(0.5*len(batch_pos))\n",
    "    for x_ind in range(len(batch_pos)):  # the length of batch_pos is the number of unrrolings\n",
    "        # Concatenate positive and negative batches and arrange them according to rand_ind\n",
    "        X.append( np.concatenate((batch_pos[x_ind],batch_neg[x_ind]),0)[rand_ind,:] )\n",
    "        \n",
    "    return X, y_aux\n",
    "\n",
    "valid_X, valid_y = Get_Val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lstm_model_V2 import *\n",
    "\n",
    "class myLstmNet(LstmNet):\n",
    "    def get_extra_inputs(self, i, h_list, state_list):\n",
    "        #print('OK:', len(h_list))\n",
    "        return i\n",
    "    \n",
    "    def evaluate_final_output(self, outputs_list, inputs_list, h_list ):\n",
    "        ''' Calculates the final output of the neural network, usually it is just a linear transformation\n",
    "        \n",
    "        outputs_list: list with the outputs from the last lstm cell\n",
    "        inputs_list: list of inputs to the network\n",
    "        h_list: list with all hidden outputs from all the cells\n",
    "        '''\n",
    "        ''''''\n",
    "        all_hidden = list()\n",
    "        #print('n_unrollings:', len(h_list)) # DELETE !!!\n",
    "        #print('n_layers:', len(h_list[0])) # DELETE !!!\n",
    "        \n",
    "        for t in h_list: # go trough each time step\n",
    "            all_hidden.append( tf.concat(1,t) )\n",
    "        return self.out_layer.evaluate(tf.concat(0, all_hidden))  \n",
    "        \n",
    "        \n",
    "        # Original:\n",
    "        #return self.out_layer.evaluate(tf.concat(0, outputs_list))\n",
    "    \n",
    "if len(num_nodes)>1:\n",
    "    n_extra= [num_inputs for i in range(len(num_nodes)+1)]\n",
    "    n_extra[0]= 0\n",
    "    n_extra[-1]= sum(num_nodes) - num_nodes[-1]\n",
    "else:\n",
    "    n_extra= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    \n",
    "    def __init__( self, pos_net, neg_net, w, b, batch_size, num_unrollings, drop_prob_list, name=''):\n",
    "    \n",
    "        # 1. Create placeholders for inputs \n",
    "        self.X = list()\n",
    "        for iaux in range(num_unrollings + 1):\n",
    "            self.X.append(tf.placeholder(tf.float32, shape=[batch_size, vc.vocabulary_size], \n",
    "                                             name= name+'X_i'+str(iaux)+'_All'))\n",
    "        aux_inputs = self.X[:num_unrollings]\n",
    "        aux_labels = self.X[1:]  # inputs shifted by one time step.\n",
    "\n",
    "        # Create a list for store the placeholders for the labels\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "       \n",
    "\n",
    "        # -------------------- unrolling of the network --------------------------- # \n",
    "        self.pos_unroll, _= pos_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   )\n",
    "\n",
    "        self.neg_unroll, _= neg_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        # error_per_sample is a vector, its shape is changed to have each unrolling in separate columns\n",
    "        output_pos= tf.reshape(self.pos_unroll.error_per_sample,[num_unrollings,batch_size])  \n",
    "        output_neg= tf.reshape(self.neg_unroll.error_per_sample,[num_unrollings,batch_size]) \n",
    "\n",
    "        output_pos_mean= tf.reduce_mean(output_pos, reduction_indices= 0) \n",
    "        output_neg_mean= tf.reduce_mean(output_neg, reduction_indices= 0) \n",
    "\n",
    "        output_mean= tf.transpose(tf.pack( [ output_pos_mean, output_neg_mean ]))\n",
    "\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "\n",
    "\n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        \n",
    "        # prediction error\n",
    "        #Lp = tf.reduce_mean( tf.mul(self.labels, output_pos_mean) + tf.mul(self.labels-1, output_neg_mean))\n",
    "        Lp = tf.reduce_mean( tf.mul(tf.squeeze(self.labels), output_pos_mean) + \n",
    "                             tf.mul(tf.squeeze(1-self.labels), output_neg_mean)\n",
    "                           )\n",
    "        # counter-prediction penalty\n",
    "        Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.exp(-output_pos_mean)) + \n",
    "                              tf.mul(tf.squeeze(self.labels), tf.exp(-output_neg_mean))\n",
    "                            )\n",
    "        # regularization\n",
    "        l2_c= tf.nn.l2_loss(w)\n",
    "        \n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp + beta*(1.0/Lcp)\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp - beta*(Lcp)\n",
    "        self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp + beta*Lcp + lambda_w*l2_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # For dropout\n",
    "    drop_prob = tf.placeholder(tf.float32)\n",
    "    drop_prob_list = [ drop_prob for i in range(len(num_nodes)+1)]\n",
    "    drop_prob_list[0]= None\n",
    "    \n",
    "     \n",
    "    # 1. Define positive and negative neural networks\n",
    "    pos_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= pos_net_name)\n",
    "        \n",
    "    neg_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= neg_net_name)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    #w = tf.Variable(tf.truncated_normal([2, 1], -0.1, 0.1), name=('w_class')) # unconstrained w\n",
    "    #w = tf.Variable(tf.constant( [[-2.0], [2.0]]), name=('w_class'), trainable=False) # fixed w\n",
    "    \n",
    "    #w = tf.Variable(tf.truncated_normal([1, 1], 0.3, 0.4), name=('w_class')) # unconstrained w\n",
    "    w = tf.Variable(tf.constant( [[0.3]] ), name=('w_class')) # fixed w\n",
    "    w = tf.concat(0, [-w, w])\n",
    "    \n",
    "    b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Define unrolling for training\n",
    "    train= ModelSetup( pos_net, neg_net, w, b, batch_size, num_unrollings, \n",
    "                       drop_prob_list, \n",
    "                       name='train_')\n",
    "       \n",
    "    # 3. Define unrolling for validation\n",
    "    valid= ModelSetup( pos_net, neg_net, w, b, batch_size_val, num_unrollings_val, \n",
    "                       drop_prob_list= [None, None, None, None, None], \n",
    "                       name='train_')\n",
    "   \n",
    "    # 4. Define unrolling for testing\n",
    "    pos_gen_test, _= pos_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    neg_gen_test, _= neg_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    \n",
    "          \n",
    "    \n",
    "    # 5. Define optimizer    \n",
    "    # 1. specify the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) # ADAM 0.001\n",
    "    \n",
    "    # 2. get the gradients and variables\n",
    "    # grads_and_vars is a list of tuples (gradient, variable). \n",
    "    grads_and_vars = optimizer.compute_gradients(train.loss) \n",
    "    gradients, v = zip(*grads_and_vars)\n",
    "    \n",
    "    # 3. process the gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, grad_clip_thresh)  #1.25 #0.025 #0.001(last used)\n",
    "    # 4. apply the gradients to the optimization procedure\n",
    "    optimizer = optimizer.apply_gradients( zip(gradients, v) ) # ADAM\n",
    "    \n",
    "    \n",
    "    # Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Energy predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_pos= False\n",
    "load_pos_file=  working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_pos.ckpt\"\n",
    "    \n",
    "reload_neg= False\n",
    "load_neg_file= working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_neg.ckpt\"\n",
    "\n",
    "reload_all= True\n",
    "load_all_file=  working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run-1) +\"r_All\"+comment+\".ckpt\";\n",
    "\n",
    "save_all= True\n",
    "save_all_file=  working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run) +\"r_All\"+comment+\".ckpt\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_run==1:\n",
    "    reload_all= False;\n",
    "else:\n",
    "    reload_pos= False;\n",
    "    reload_neg= False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Initialized\n",
      "step | Train_err, Train_mis | Valid_err, Valid_mis | loss\n",
      "0 | 0.515625, 33 | 0.562500, 36 | 0.731230 | w:[-0.300100,0.300100], b:0.000000\n",
      "50 | 0.453125, 29 | 0.453125, 29 | 0.728945 | w:[-0.298915,0.298915], b:0.000000\n",
      "100 | 0.453125, 29 | 0.390625, 25 | 0.724589 | w:[-0.298561,0.298561], b:0.000000\n",
      "150 | 0.453125, 29 | 0.437500, 28 | 0.722337 | w:[-0.300063,0.300063], b:0.000000\n",
      "200 | 0.546875, 35 | 0.468750, 30 | 0.721327 | w:[-0.302206,0.302206], b:0.000000\n",
      "250 | 0.515625, 33 | 0.406250, 26 | 0.721222 | w:[-0.304880,0.304880], b:0.000000\n",
      "300 | 0.453125, 29 | 0.546875, 35 | 0.720682 | w:[-0.307684,0.307684], b:0.000000\n",
      "350 | 0.390625, 25 | 0.484375, 31 | 0.720408 | w:[-0.311387,0.311387], b:0.000000\n",
      "400 | 0.390625, 25 | 0.578125, 37 | 0.720855 | w:[-0.314017,0.314017], b:0.000000\n",
      "450 | 0.406250, 26 | 0.468750, 30 | 0.720155 | w:[-0.317306,0.317306], b:0.000000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 13000 \n",
    "summary_frequency = 50\n",
    "n_valid_tests = 100\n",
    "n_characters_step= num_unrollings*batch_size\n",
    "\n",
    "aux_print=0\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # -------------------------------- Load weigths from file, or initialize variables ------------------------------------\n",
    "    if reload_all:\n",
    "        saver.restore(session, load_all_file)\n",
    "        #session.run( global_step.assign(0) ) # SGD\n",
    "        \n",
    "    elif (reload_pos and reload_neg):\n",
    "        tf.initialize_all_variables().run()\n",
    "        pos_net.saver.restore(session, load_pos_file)\n",
    "        neg_net.saver.restore(session, load_neg_file)\n",
    "        print('Weights for positive and negative networks loaded')\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Weights Initialized')\n",
    "    \n",
    "    # ------------------------------------------- Training loop --------------------------------------------------\n",
    "    print('step | Train_err, Train_mis | Valid_err, Valid_mis | loss')\n",
    "    \n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        # 1. Get next batch\n",
    "        batch_X, batch_y = GetNextBatch_sentiment()        \n",
    "        \n",
    "        # 2. Setup feed dictionary for network 1 and network 2\n",
    "        feed_dict = dict()\n",
    "        # For dropout\n",
    "        feed_dict[drop_prob] = dropout_cons\n",
    "        # Inroduce labels\n",
    "        feed_dict[train.labels] = batch_y\n",
    "        # Introduce inputs\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train.X[i]] = batch_X[i]\n",
    "                               \n",
    "        # 3 Run optimizer\n",
    "        #_, l, lr = session.run( [optimizer, loss_train, learning_rate], feed_dict=feed_dict) # SGD\n",
    "        _, l = session.run( [optimizer, train.loss], feed_dict=feed_dict) # ADAM\n",
    "        \n",
    "        mean_loss += l\n",
    "        \n",
    "        # --------------------------------------------- logging ------------------------------------------------\n",
    "        if train_size_pos<aux_print :\n",
    "            print('+'*80)\n",
    "            print('')\n",
    "            aux_print = 0\n",
    "        else:\n",
    "            aux_print += n_characters_step\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                                \n",
    "            # ---------- Print loss in validation dataset ---------\n",
    "            if Allow_Valid:\n",
    "                '''train mis classification'''        \n",
    "                prediction_train = session.run( [tf.nn.sigmoid(train.logits)], feed_dict=feed_dict)          \n",
    "                mis_pred_train = np.sum( np.not_equal( np.greater(prediction_train, 0.5), np.greater(batch_y, 0.5)) )\n",
    "                \n",
    "                \n",
    "                '''valid mis classification'''\n",
    "                mis_pred_val = 0\n",
    "                for step_valid in range(n_valid_tests):\n",
    "                    # 1. Get next batch\n",
    "                    batch_X_val, batch_y_val = Get_Val()        \n",
    "                    feed_dict_val = dict()\n",
    "                    # For dropout\n",
    "                    feed_dict_val[drop_prob] = 1.0\n",
    "                    # Inroduce labels\n",
    "                    feed_dict_val[valid.labels] = batch_y_val\n",
    "                    # Introduce inputs\n",
    "                    for i in range(num_unrollings_val+1):\n",
    "                        feed_dict_val[valid.X[i]] = batch_X_val[i]\n",
    "                    # 2. Get prediction\n",
    "                    prediction_val = session.run( [tf.nn.sigmoid(valid.logits)], feed_dict=feed_dict_val)          \n",
    "                    mis_pred_val = mis_pred_val*step_valid\n",
    "                    mis_pred_val += np.sum( np.not_equal( np.greater(prediction_val, 0.5), np.greater(batch_y_val, 0.5)) )\n",
    "                    mis_pred_val = mis_pred_val/(step_valid+1)\n",
    "                    #\n",
    "                    if (mis_pred_val>0.3) and (step_valid==0):\n",
    "                        break                        \n",
    "                    elif (step_valid==0):\n",
    "                        print('Low validation error reached:', mis_pred_val)\n",
    "                \n",
    "                ''' weights and bias for classficator '''\n",
    "                # print values for b and w\n",
    "                b_t= b.eval()\n",
    "                w_t= w.eval()\n",
    "                \n",
    "                print('%d | %f, %d | %f, %d | %f | w:[%f,%f], b:%f' % (step, \n",
    "                                                     mis_pred_train/batch_y.shape[0], mis_pred_train,\n",
    "                                                     mis_pred_val/batch_y_val.shape[0], mis_pred_val,\n",
    "                                                     mean_loss,\n",
    "                                                     w_t[0], w_t[1], b_t[0]\n",
    "                                                    ))                \n",
    "                #if(mis_pred_val/batch_y_val.shape[0] < 0.20):\n",
    "                #    break\n",
    "                \n",
    "            mean_loss = 0\n",
    "            \n",
    "    if save_all:\n",
    "        save_path = saver.save(session, save_all_file)\n",
    "        print(\"LSTM cell Model for negative reviews saved in file: %s\" % save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some samples from positive and negative networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Load weigths from file, or initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(session, save_all_file)\n",
    "    print('Weights loaded')\n",
    "        \n",
    "    # print values for b and w\n",
    "    b_t= b.eval()\n",
    "    w_t= w.eval()\n",
    "    #b_t= session.run(b)\n",
    "    print('b=', b_t)\n",
    "    print('w=', w_t)\n",
    "    \n",
    "    \n",
    "    # For positive network\n",
    "    print('-'*30 + 'pos' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)[0]\n",
    "        pos_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(70):\n",
    "            prediction = tf.nn.softmax(pos_gen_test.y).eval({pos_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)[0]\n",
    "        print(sentence)\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # For negative network\n",
    "    print('-'*30 + 'neg' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)[0]\n",
    "        neg_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(70):\n",
    "            prediction = tf.nn.softmax(neg_gen_test.y).eval({neg_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)[0]\n",
    "        print(sentence)\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
