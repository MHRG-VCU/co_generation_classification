{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pickle\n",
    "import math\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from twodlearn.tf_lib.Feedforward import LinearLayer\n",
    "\n",
    "import sys\n",
    "working_dir= '/home/marinodl/dl_projects/nlp/sentiment_analysis/sa_by_word/'\n",
    "sys.path.insert(0, working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allow_valid= True\n",
    "Valid_Percentage= 0.1 # i.e. 10% of the data will be used for validation\n",
    "\n",
    "pos_net_name= '_pos'\n",
    "neg_net_name= '_neg'\n",
    "\n",
    "activation_function='tanh'\n",
    "num_nodes = [300, 300, 300] #64 500, num_nodes: Nodes for the LSTM cell\n",
    "alpha = 0.0 #0.1 #0.1\n",
    "beta = 0.0 #0.01 #10000.01\n",
    "lambda_w = 0.00000001\n",
    "\n",
    "dropout_cons = 1.0\n",
    "\n",
    "Allow_Bias= True \n",
    "\n",
    "learning_rate= 0.0001      # 0.001\n",
    "grad_clip_thresh= 0.001       # 0.00001\n",
    "\n",
    "current_run= 2\n",
    "batch_size= 64 #64\n",
    "num_unrollings= 64 #100\n",
    "\n",
    "batch_size_val= 64 #len(valid_text_pos)/num_unrollings_val #500\n",
    "\n",
    "comment='_01alpha_noDropout_LcLpLcp_100unrol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: [300, 300, 300]\n",
      "alpha: 0.0 , beta: 0.0 , lambda_w: 1e-08\n",
      "num_unrollings: 64 , batch_size: 64 , batch_size_val: 64\n",
      "learning_rate: 0.0001 grad_clip_thresh: 0.01\n"
     ]
    }
   ],
   "source": [
    "model_version = 'L'+str(len(num_nodes))\n",
    "print(\"num_nodes:\",num_nodes)\n",
    "print(\"alpha:\",alpha, \", beta:\",beta,\", lambda_w:\", lambda_w)\n",
    "print(\"num_unrollings:\",num_unrollings, \", batch_size:\",batch_size,\", batch_size_val:\", batch_size_val)\n",
    "print(\"learning_rate:\", learning_rate, 'grad_clip_thresh:', grad_clip_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vocabulary_coding_simple import *\n",
    "\n",
    "vc= pickle.load( open( \"imdb_vc.pkl\", \"rb\" ) )\n",
    "num_inputs=  vc.vocabulary_size\n",
    "num_outputs= vc.vocabulary_size\n",
    "\n",
    "text_pos= pickle.load( open( \"imdb_text_pos.pkl\", \"rb\" ) )\n",
    "text_neg= pickle.load( open( \"imdb_text_neg.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 93, 1, 12]\n",
      "i love the movi \n"
     ]
    }
   ],
   "source": [
    "print(vc.text2keys(['i','loved','the','movie']))\n",
    "print(vc.keys2text([11,93,1,12]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs: 1001\n",
      "text length: 2264691\n"
     ]
    }
   ],
   "source": [
    "print('# inputs:',num_inputs)\n",
    "\n",
    "len_text= min(len(text_pos), len(text_neg)) #CAREFUL\n",
    "print('text length:',len_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226469\n",
      "2038222\n"
     ]
    }
   ],
   "source": [
    "valid_size_pos = int(math.floor(len_text)*0.1)\n",
    "valid_text_pos = text_pos[:valid_size_pos]\n",
    "train_text_pos = text_pos[valid_size_pos:len_text] # excluding validation set\n",
    "#train_text_pos = text_pos[:len_text]               # including validation set\n",
    "train_size_pos = len(train_text_pos)\n",
    "\n",
    "print(valid_size_pos)\n",
    "print(train_size_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2038222\n"
     ]
    }
   ],
   "source": [
    "valid_size_neg = int(math.floor(len_text*0.1))\n",
    "valid_text_neg = text_neg[:valid_size_neg]\n",
    "train_text_neg = text_neg[valid_size_neg:len_text] # excluding validation set\n",
    "#train_text_neg = text_neg[:len_text]               # including validation set\n",
    "train_size_neg = len(train_text_neg)               \n",
    "print(train_size_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10 # this is to prevent that log() returns minus infinity\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vc.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vc.vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get supervised-batch from positive and negative texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches_pos = BatchGenerator(train_text_pos, batch_size//2, num_unrollings, vc)\n",
    "batches_neg = BatchGenerator(train_text_neg, batch_size//2, num_unrollings, vc)\n",
    "\n",
    "def GetNextBatch_sentiment(rand_shuffle= False):\n",
    "    ''' Generates a batch of sentences and its corresponding labels for classification\n",
    "        Returns:\n",
    "            X: list of positive and negative sentences, the format is the same that the batches generated from the \n",
    "               positive and negative texts. len(X)= num_unrrolings, X[i].shape: [batch_size x vocabulary_size]\n",
    "            y: class to which each sentence belong. The format is a vector of size [batch_size x 1]\n",
    "    '''\n",
    "    batch_pos = batches_pos.next()\n",
    "    batch_neg = batches_neg.next()\n",
    "        \n",
    "    y_aux= np.zeros(shape=[batch_size,1], dtype=np.float) # y_aux= class for each one of the sentences\n",
    "    y_aux[0:int(batch_size//2),0]= 1 # positive samples are denoted by 1, negative by 0\n",
    "    \n",
    "    # TODO: ADD MULTIPLE CLASSES\n",
    "    if rand_shuffle:\n",
    "        rand_ind= np.random.permutation(batch_size) # rand_ind= random shuffling for the sentences\n",
    "        y_aux=y_aux[rand_ind,:]\n",
    "    \n",
    "    X= list()\n",
    "    #y= list()\n",
    "    # TODO: try to do not penalize mistakes being done in the first samples of the unrolling\n",
    "    # start_ind= math.floor(0.5*len(batch_pos))\n",
    "    for x_ind in range(len(batch_pos)):  # the length of batch_pos is the number of unrollings\n",
    "        # Concatenate positive and negative batches and arrange them according to rand_ind\n",
    "        if rand_shuffle:\n",
    "            X.append( np.concatenate((batch_pos[x_ind],batch_neg[x_ind]),0)[rand_ind,:] )\n",
    "        else:\n",
    "            X.append( np.concatenate((batch_pos[x_ind],batch_neg[x_ind]),0) )\n",
    "        \n",
    "    return X, y_aux\n",
    "\n",
    "batch_X, batch_y = GetNextBatch_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size_val: 64  suggested: 3538.578125\n"
     ]
    }
   ],
   "source": [
    "num_unrollings_val= num_unrollings #10 #num_unrollings \n",
    "print('batch_size_val:', batch_size_val, ' suggested:', len(valid_text_pos)/num_unrollings_val)\n",
    "\n",
    "batches_pos_val = BatchGenerator(valid_text_pos, batch_size_val//2, num_unrollings_val, vc)\n",
    "batches_neg_val = BatchGenerator(valid_text_neg, batch_size_val//2, num_unrollings_val, vc)\n",
    "#batches_pos_val = BatchGenerator(train_text_pos, batch_size_val/2, num_unrollings_val)\n",
    "#batches_neg_val = BatchGenerator(train_text_neg, batch_size_val/2, num_unrollings_val)\n",
    "\n",
    "\n",
    "def Get_Val():\n",
    "    batch_pos = batches_pos_val.next()\n",
    "    batch_neg = batches_neg_val.next()\n",
    "        \n",
    "    y_aux= np.zeros(shape=[batch_size_val,1], dtype=np.float) # y_aux= class for each one of the sentences\n",
    "    y_aux[0:int(batch_size_val//2),0]= 1\n",
    "    \n",
    "    # TODO: ADD MULTIPLE CLASSES\n",
    "    rand_ind= np.random.permutation(batch_size_val) # rand_ind= random shuffling for the sentences\n",
    "    \n",
    "    y_aux=y_aux[rand_ind,:]\n",
    "    \n",
    "    X= list()\n",
    "    #y= list()\n",
    "    # TODO: try to do not penalize mistakes being done in the first samples of the unrolling\n",
    "    # start_ind= math.floor(0.5*len(batch_pos))\n",
    "    for x_ind in range(len(batch_pos)):  # the length of batch_pos is the number of unrrolings\n",
    "        # Concatenate positive and negative batches and arrange them according to rand_ind\n",
    "        X.append( np.concatenate((batch_pos[x_ind],batch_neg[x_ind]),0)[rand_ind,:] )\n",
    "        \n",
    "    return X, y_aux\n",
    "\n",
    "valid_X, valid_y = Get_Val()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lstm_model_V2 import *\n",
    "\n",
    "class myLstmNet(LstmNet):\n",
    "    def get_extra_inputs(self, i, h_list, state_list):\n",
    "        #print('OK:', len(h_list))\n",
    "        return i\n",
    "    \n",
    "    def evaluate_final_output(self, outputs_list, inputs_list, h_list ):\n",
    "        ''' Calculates the final output of the neural network, usually it is just a linear transformation\n",
    "        \n",
    "        outputs_list: list with the outputs from the last lstm cell\n",
    "        inputs_list: list of inputs to the network\n",
    "        h_list: list with all hidden outputs from all the cells\n",
    "        '''\n",
    "        ''''''\n",
    "        all_hidden = list()\n",
    "        #print('n_unrollings:', len(h_list)) # DELETE !!!\n",
    "        #print('n_layers:', len(h_list[0])) # DELETE !!!\n",
    "        \n",
    "        for t in h_list: # go trough each time step\n",
    "            all_hidden.append( tf.concat(1,t) )\n",
    "        return self.out_layer.evaluate(tf.concat(0, all_hidden))  \n",
    "        \n",
    "        \n",
    "        # Original:\n",
    "        #return self.out_layer.evaluate(tf.concat(0, outputs_list))\n",
    "    \n",
    "if len(num_nodes)>1:\n",
    "    n_extra= [num_inputs for i in range(len(num_nodes)+1)]\n",
    "    n_extra[0]= 0\n",
    "    n_extra[-1]= sum(num_nodes) - num_nodes[-1]\n",
    "else:\n",
    "    n_extra= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    \n",
    "    def __init__( self, pos_net, neg_net, w, b, batch_size, num_unrollings, drop_prob_list, name=''):\n",
    "    \n",
    "        # 1. Create placeholders for inputs \n",
    "        self.X = list()\n",
    "        for iaux in range(num_unrollings + 1):\n",
    "            self.X.append(tf.placeholder(tf.float32, shape=[batch_size, vc.vocabulary_size], \n",
    "                                             name= name+'X_i'+str(iaux)+'_All'))\n",
    "        aux_inputs = self.X[:num_unrollings]\n",
    "        aux_labels = self.X[1:]  # inputs shifted by one time step.\n",
    "\n",
    "        # Create a list for store the placeholders for the labels\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "       \n",
    "\n",
    "        # -------------------- unrolling of the network --------------------------- # \n",
    "        self.pos_unroll, _= pos_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   )\n",
    "\n",
    "        self.neg_unroll, _= neg_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        y_pos= tf.reshape(self.pos_unroll.y, [-1, batch_size, num_outputs])\n",
    "        y_neg= tf.reshape(self.neg_unroll.y, [-1, batch_size, num_outputs])\n",
    "        \n",
    "        y_pos_mean= tf.reduce_mean(y_pos, reduction_indices=0)\n",
    "        y_neg_mean= tf.reduce_mean(y_neg, reduction_indices=0)\n",
    "        \n",
    "        output_mean= tf.concat(1, [ y_pos_mean, y_neg_mean ])\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "        \n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        \n",
    "        # prediction error\n",
    "        #E_pos= tf.reshape(self.pos_unroll.error_per_sample,[num_unrollings,batch_size])  \n",
    "        #E_neg= tf.reshape(self.neg_unroll.error_per_sample,[num_unrollings,batch_size]) \n",
    "        #\n",
    "        #E_pos_mean= tf.reduce_mean(E_pos, reduction_indices= 0) \n",
    "        #E_neg_mean= tf.reduce_mean(E_neg, reduction_indices= 0)\n",
    "        #\n",
    "        #Lp = tf.reduce_mean( tf.mul(self.labels, output_pos_mean) + tf.mul(self.labels-1, output_neg_mean))\n",
    "        #Lp = tf.reduce_mean( tf.mul(tf.squeeze(self.labels), E_pos_mean) + \n",
    "        #                     tf.mul(tf.squeeze(1-self.labels), E_neg_mean)\n",
    "        #                   )\n",
    "        # counter-prediction penalty\n",
    "        #\n",
    "        #Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.exp(-E_pos_mean)) + \n",
    "        #                      tf.mul(tf.squeeze(self.labels), tf.exp(-E_neg_mean))\n",
    "        #                    )\n",
    "        #\n",
    "        #Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.reduce_mean(tf.exp(-E_pos), reduction_indices= 0) ) + \n",
    "        #                      tf.mul(tf.squeeze(self.labels), tf.reduce_mean(tf.exp(-E_neg), reduction_indices= 0) )\n",
    "        #                    )\n",
    "        \n",
    "        # regularization\n",
    "        l2_c= tf.nn.l2_loss(w)\n",
    "        \n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp + beta*(1.0/Lcp)\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp - beta*(Lcp)\n",
    "        #self.alpha_r = tf.placeholder(tf.float32)\n",
    "        #self.beta_r = tf.placeholder(tf.float32)\n",
    "        \n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) #+ self.alpha_r*Lp + self.beta_r*Lcp + lambda_w*l2_c\n",
    "        self.loss = tf.reduce_mean(self.error_per_sample) #+ lambda_w*l2_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # For dropout\n",
    "    drop_prob = tf.placeholder(tf.float32)\n",
    "    drop_prob_list = [ drop_prob for i in range(len(num_nodes)+1)]\n",
    "    drop_prob_list[0]= None\n",
    "     \n",
    "    # 1. Define positive and negative neural networks\n",
    "    pos_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= pos_net_name)\n",
    "        \n",
    "    neg_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= neg_net_name)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_outputs*2, 1], -0.1, 0.1), name=('w_class')) # unconstrained w\n",
    "    #w = tf.Variable(tf.constant( [[-2.0], [2.0]]), name=('w_class'), trainable=False) # fixed w\n",
    "    \n",
    "    #w = tf.Variable(tf.truncated_normal([1, 1], 0.3, 0.4), name=('w_class')) # unconstrained w\n",
    "    #w = tf.Variable(tf.constant( [[0.3]] ), name=('w_class')) # fixed w\n",
    "    #w = tf.concat(0, [-w, w])\n",
    "    \n",
    "    b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Define unrolling for training\n",
    "    train= ModelSetup( pos_net, neg_net, w, b, batch_size, num_unrollings, \n",
    "                       drop_prob_list, \n",
    "                       name='train_')\n",
    "       \n",
    "    # 3. Define unrolling for validation\n",
    "    if allow_valid:\n",
    "        valid= ModelSetup( pos_net, neg_net, w, b, batch_size_val, num_unrollings_val, \n",
    "                           drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                           name='valid_')\n",
    "\n",
    "    # 4. Define unrolling for testing\n",
    "    pos_gen_test, _= pos_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    neg_gen_test, _= neg_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    \n",
    "          \n",
    "    \n",
    "    # 5. Define optimizer    \n",
    "    # 1. specify the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) # ADAM 0.001\n",
    "    \n",
    "    # 2. get the gradients and variables\n",
    "    # grads_and_vars is a list of tuples (gradient, variable). \n",
    "    grads_and_vars = optimizer.compute_gradients(train.loss) \n",
    "    gradients, v = zip(*grads_and_vars)\n",
    "    \n",
    "    # 3. process the gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, grad_clip_thresh)  #1.25 #0.025 #0.001(last used)\n",
    "    # 4. apply the gradients to the optimization procedure\n",
    "    optimizer = optimizer.apply_gradients( zip(gradients, v) ) # ADAM\n",
    "    \n",
    "    # for prediction\n",
    "    train_pred = tf.nn.sigmoid(train.logits)\n",
    "    \n",
    "    # Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Energy predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_pos= False\n",
    "load_pos_file=  working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_pos.ckpt\"\n",
    "    \n",
    "reload_neg= False\n",
    "load_neg_file= working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_neg.ckpt\"\n",
    "\n",
    "reload_all= True\n",
    "load_all_file=  working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run-1) +\"r_All\"+comment+\".ckpt\";\n",
    "\n",
    "save_all= True\n",
    "save_all_file=  working_dir+\"weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run) +\"r_All\"+comment+\".ckpt\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_run==1:\n",
    "    reload_all= False;\n",
    "else:\n",
    "    reload_pos= False;\n",
    "    reload_neg= False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step | Train_err, Train_mis | Valid_err, Valid_mis | loss\n",
      "0 | 0.203125 (0.250000), 13 | 0.240937, 15 | 0.555719 | w:[-0.063232,-0.123253], b:-0.000049\n",
      "50 | 0.281250 (0.249375), 18 | 0.256875, 16 | 0.506943 | w:[-0.063232,-0.123253], b:-0.000049\n",
      "100 | 0.265625 (0.248438), 17 | 0.220938, 14 | 0.497953 | w:[-0.063232,-0.123253], b:-0.000049\n",
      "150 | 0.234375 (0.244062), 15 | 0.270625, 17 | 0.488840 | w:[-0.063232,-0.123253], b:-0.000048\n",
      "200 | 0.218750 (0.219375), 14 | 0.229687, 14 | 0.460236 | w:[-0.063233,-0.123254], b:-0.000048\n",
      "250 | 0.171875 (0.230313), 11 | 0.253437, 16 | 0.489941 | w:[-0.063233,-0.123254], b:-0.000048\n",
      "300 | 0.250000 (0.234063), 16 | 0.247500, 15 | 0.487042 | w:[-0.063233,-0.123254], b:-0.000048\n",
      "350 | 0.296875 (0.233750), 19 | 0.249062, 15 | 0.476599 | w:[-0.063233,-0.123254], b:-0.000048\n",
      "400 | 0.234375 (0.251250), 15 | 0.251250, 16 | 0.498981 | w:[-0.063233,-0.123254], b:-0.000048\n",
      "450 | 0.218750 (0.241875), 14 | 0.243750, 15 | 0.499500 | w:[-0.063233,-0.123254], b:-0.000048\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "500 | 0.218750 (0.235000), 14 | 0.265312, 16 | 0.486950 | w:[-0.063233,-0.123254], b:-0.000047\n",
      "550 | 0.328125 (0.235625), 21 | 0.245625, 15 | 0.494299 | w:[-0.063233,-0.123255], b:-0.000047\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000 \n",
    "summary_frequency = 50\n",
    "n_valid_tests = 50\n",
    "n_characters_step= num_unrollings*batch_size\n",
    "\n",
    "aux_print=0\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # -------------------------------- Load weigths from file, or initialize variables ------------------------------------\n",
    "    if reload_all:\n",
    "        saver.restore(session, load_all_file)\n",
    "        #session.run( global_step.assign(0) ) # SGD\n",
    "        \n",
    "    elif (reload_pos and reload_neg):\n",
    "        tf.initialize_all_variables().run()\n",
    "        pos_net.saver.restore(session, load_pos_file)\n",
    "        neg_net.saver.restore(session, load_neg_file)\n",
    "        print('Weights for positive and negative networks loaded')\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Weights Initialized')\n",
    "    \n",
    "    # ------------------------------------------- Training loop --------------------------------------------------\n",
    "    print('step | Train_err, Train_mis | Valid_err, Valid_mis | loss')\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    mean_mis_pred = 0.0\n",
    "    for step in range(num_steps):\n",
    "        # 1. Get next batch\n",
    "        batch_X, batch_y = GetNextBatch_sentiment()        \n",
    "        \n",
    "        # 2. Setup feed dictionary for network 1 and network 2\n",
    "        feed_dict = dict()\n",
    "        # For dropout\n",
    "        feed_dict[drop_prob] = dropout_cons\n",
    "        # hyperparameters\n",
    "        '''\n",
    "        if step<200:\n",
    "            feed_dict[train.alpha_r] = 1.0\n",
    "            feed_dict[train.beta_r] = 0.0\n",
    "        else:\n",
    "            feed_dict[train.alpha_r] = alpha\n",
    "            feed_dict[train.beta_r] = beta\n",
    "        '''    \n",
    "        # Inroduce labels\n",
    "        feed_dict[train.labels] = batch_y\n",
    "        # Introduce inputs\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train.X[i]] = batch_X[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 3 Run optimizer\n",
    "        #_, l, lr = session.run( [optimizer, loss_train, learning_rate], feed_dict=feed_dict) # SGD\n",
    "        _, l, pred_train_aux = session.run( [optimizer, train.loss, train_pred], feed_dict=feed_dict) # ADAM\n",
    "        \n",
    "        mean_mis_pred += np.sum( np.not_equal( np.greater(pred_train_aux, 0.5), np.greater(batch_y, 0.5)) )\n",
    "        mean_loss += l\n",
    "        \n",
    "        # --------------------------------------------- logging ------------------------------------------------\n",
    "        if train_size_pos<aux_print :\n",
    "            print('+'*80)\n",
    "            print('')\n",
    "            aux_print = 0\n",
    "        else:\n",
    "            aux_print += n_characters_step\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                mean_mis_pred = mean_mis_pred / summary_frequency\n",
    "            \n",
    "            '''train mis classification'''        \n",
    "            prediction_train = session.run( [tf.nn.sigmoid(train.logits)], feed_dict=feed_dict)          \n",
    "            mis_pred_train = np.sum( np.not_equal( np.greater(prediction_train, 0.5), np.greater(batch_y, 0.5)) )\n",
    "            \n",
    "            # ---------- Print loss in validation dataset ---------\n",
    "            if allow_valid:\n",
    "                '''valid mis classification'''\n",
    "                mis_pred_val = 0\n",
    "                for step_valid in range(n_valid_tests):\n",
    "                    # 1. Get next batch\n",
    "                    batch_X_val, batch_y_val = Get_Val()        \n",
    "                    feed_dict_val = dict()\n",
    "                    # For dropout\n",
    "                    feed_dict_val[drop_prob] = 1.0\n",
    "                    # Inroduce labels\n",
    "                    feed_dict_val[valid.labels] = batch_y_val\n",
    "                    # Introduce inputs\n",
    "                    for i in range(num_unrollings_val+1):\n",
    "                        feed_dict_val[valid.X[i]] = batch_X_val[i]\n",
    "                    # 2. Get prediction\n",
    "                    prediction_val = session.run( [tf.nn.sigmoid(valid.logits)], feed_dict=feed_dict_val)          \n",
    "                    mis_pred_val = mis_pred_val*step_valid\n",
    "                    mis_pred_val += np.sum( np.not_equal( np.greater(prediction_val, 0.5), np.greater(batch_y_val, 0.5)) )\n",
    "                    mis_pred_val = mis_pred_val/(step_valid+1)\n",
    "                    #Delete from here\n",
    "                    #print('err=', np.sum( np.not_equal( np.greater(prediction_val, 0.5), np.greater(batch_y_val, 0.5)) ))\n",
    "                    #to here\n",
    "                    \n",
    "                    #if ((mis_pred_val/batch_y_val.shape[0])>0.2) and (step_valid==0):\n",
    "                    #    break                        \n",
    "                    #elif (step_valid==0):\n",
    "                    #    break\n",
    "                    #    print('Low validation error reached:', (mis_pred_val/batch_y_val.shape[0]))\n",
    "                \n",
    "                ''' weights and bias for classficator '''\n",
    "                # print values for b and w\n",
    "                b_t= b.eval()\n",
    "                w_t= w.eval()\n",
    "                \n",
    "                print('%d | %f (%f), %d | %f, %d | %f | w:[%f,%f], b:%f' % (step, \n",
    "                                                     mis_pred_train/batch_y.shape[0], mean_mis_pred/batch_y.shape[0], mis_pred_train,\n",
    "                                                     mis_pred_val/batch_y_val.shape[0], mis_pred_val,\n",
    "                                                     mean_loss,\n",
    "                                                     w_t[0], w_t[1], b_t[0]\n",
    "                                                    ))    \n",
    "                #if(mis_pred_val/batch_y_val.shape[0] < 0.20):\n",
    "                #    break\n",
    "            else:\n",
    "                print('%d | %f, %f | %f' % (step, mis_pred_train/batch_y.shape[0], mis_pred_train, mean_loss))\n",
    "                      \n",
    "            mean_loss = 0\n",
    "            mean_mis_pred = 0\n",
    "            \n",
    "    if save_all:\n",
    "        save_path = saver.save(session, save_all_file)\n",
    "        print(\"LSTM cell Model for negative reviews saved in file: %s\" % save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some samples from positive and negative networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Load weigths from file, or initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(session, save_all_file)\n",
    "    print('Weights loaded')\n",
    "        \n",
    "    # print values for b and w\n",
    "    b_t= b.eval()\n",
    "    w_t= w.eval()\n",
    "    #b_t= session.run(b)\n",
    "    print('b=', b_t)\n",
    "    print('w=', w_t)\n",
    "    \n",
    "    \n",
    "    # For positive network\n",
    "    print('-'*30 + 'pos' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        pos_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(70):\n",
    "            prediction = tf.nn.softmax(pos_gen_test.y).eval({pos_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('-' * 80)\n",
    "    \n",
    "    # For negative network\n",
    "    print('-'*30 + 'neg' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        neg_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(70):\n",
    "            prediction = tf.nn.softmax(neg_gen_test.y).eval({neg_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('=' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
