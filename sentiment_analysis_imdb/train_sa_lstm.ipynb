{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous generation-classification using LSTM \n",
    "Implementation of the paper: Simultaneous Generation-classification using lstm \n",
    "\n",
    "Authors: Daniel L. Marino, Kasun Amarasinghe, Milos Manic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import pickle\n",
    "import math\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from twodlearn.tf_lib.Feedforward import LinearLayer\n",
    "from twodlearn.tf_lib.Recurrent import *\n",
    "\n",
    "import sys\n",
    "working_dir= os.getcwd()\n",
    "\n",
    "print('Working directory:', working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allow_valid= True\n",
    "Valid_Percentage= 0.1 # i.e. 10% of the data will be used for validation\n",
    "\n",
    "pos_net_name= '_pos'\n",
    "neg_net_name= '_neg'\n",
    "\n",
    "activation_function='tanh'\n",
    "num_nodes = [300, 300] #num_nodes: Nodes for the LSTM cell\n",
    "alpha = 10.0 #0.1 #0.1\n",
    "beta = 0.1 #0.01 #10000.01\n",
    "lambda_w = 0.00001\n",
    "\n",
    "dropout_cons = 0.8\n",
    "\n",
    "Allow_Bias= False \n",
    "\n",
    "learning_rate= 0.005      # 0.001\n",
    "grad_clip_thresh= 1.1       # 0.00001\n",
    "\n",
    "current_run= 1\n",
    "batch_size= 64 #64\n",
    "num_unrollings= 64 #100\n",
    "\n",
    "batch_size_val= 64 #len(valid_text_pos)/num_unrollings_val #500\n",
    "num_unrollings_val= num_unrollings #100\n",
    "\n",
    "comment='_noDropout_LcLpLcp_64unrol_standarloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: [300, 300]\n",
      "alpha: 10.0 , beta: 0.1 , lambda_w: 1e-05\n",
      "num_unrollings: 64 , batch_size: 100 , batch_size_val: 100\n",
      "learning_rate: 0.005 grad_clip_thresh: 1.1\n"
     ]
    }
   ],
   "source": [
    "model_version = 'L'+str(len(num_nodes))\n",
    "print(\"num_nodes:\",num_nodes)\n",
    "print(\"alpha:\",alpha, \", beta:\",beta,\", lambda_w:\", lambda_w)\n",
    "print(\"num_unrollings:\",num_unrollings, \", batch_size:\",batch_size,\", batch_size_val:\", batch_size_val)\n",
    "print(\"learning_rate:\", learning_rate, 'grad_clip_thresh:', grad_clip_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "(100, 1)\n",
      "(100, 3001)\n",
      "head are discov in the lake steve the but not exactli sharp sheriff is on the case but onli if he s not too busi chase big citi thug travel through the area halfway through the film there suddenli is an scene about a thug rob a store and kill two peopl in the process thi moment appear to take on the complet other side \n",
      "[ 0.]\n",
      "understand whi it s brilliant it stay almost entir faith to s book without be or overli and add to it a that is pure cinemat how mani film adapt of ani author s work can claim that himself even put in a cameo appear toward the end of the film and can you ask for a better than that not onli is it a \n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "vc= pickle.load( open( \"imdb_vc.pkl\", \"rb\" ) )\n",
    "num_inputs=  vc.vocabulary_size\n",
    "num_outputs= vc.vocabulary_size\n",
    "\n",
    "dataset= pickle.load( open( \"imdb_dataset.pkl\", \"rb\" ) )\n",
    "\n",
    "dataset.train.set_batch_and_unrollings(batch_size, num_unrollings)\n",
    "dataset.valid.set_batch_and_unrollings(batch_size_val, num_unrollings_val)\n",
    "\n",
    "train_x, train_y= dataset.train.next_batch()\n",
    "print(len(train_x))\n",
    "print(train_y.shape)\n",
    "print(train_x[0].shape)\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][0,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[0])\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][50,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[50])\n",
    "\n",
    "valid_x, valid_y= dataset.valid.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs: 3001\n",
      "pos train length: 2422173\n",
      "neg train length: 2422173\n",
      "pos valid length: 269130\n",
      "neg valid length: 269130\n",
      "pos test length: 2633361\n",
      "neg test length: 2633361\n"
     ]
    }
   ],
   "source": [
    "print('# inputs:',num_inputs)\n",
    "\n",
    "print('pos train length:',dataset.train.batch_generators[0]._text_size )\n",
    "print('neg train length:',dataset.train.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos valid length:',dataset.valid.batch_generators[0]._text_size )\n",
    "print('neg valid length:',dataset.valid.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos test length:',dataset.test.batch_generators[0]._text_size )\n",
    "print('neg test length:',dataset.test.batch_generators[1]._text_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10 # this is to prevent that log() returns minus infinity\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vc.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vc.vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get supervised-batch from positive and negative texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myLstmNet(LstmNet):\n",
    "    def get_extra_inputs(self, i, h_list, state_list):\n",
    "        #print('OK:', len(h_list))\n",
    "        return i\n",
    "    \n",
    "    def evaluate_final_output(self, outputs_list, inputs_list, h_list ):\n",
    "        ''' Calculates the final output of the neural network, usually it is just a linear transformation\n",
    "        \n",
    "        outputs_list: list with the outputs from the last lstm cell\n",
    "        inputs_list: list of inputs to the network\n",
    "        h_list: list with all hidden outputs from all the cells\n",
    "        '''\n",
    "        ''''''\n",
    "        all_hidden = list()\n",
    "        #print('n_unrollings:', len(h_list)) # DELETE !!!\n",
    "        #print('n_layers:', len(h_list[0])) # DELETE !!!\n",
    "        \n",
    "        for t in h_list: # go trough each time step\n",
    "            all_hidden.append( tf.concat(1,t) )\n",
    "        return self.out_layer.evaluate(tf.concat(0, all_hidden))  \n",
    "        \n",
    "        \n",
    "        # Original:\n",
    "        #return self.out_layer.evaluate(tf.concat(0, outputs_list))\n",
    "    \n",
    "if len(num_nodes)>1:\n",
    "    n_extra= [num_inputs for i in range(len(num_nodes)+1)]\n",
    "    n_extra[0]= 0\n",
    "    n_extra[-1]= sum(num_nodes) - num_nodes[-1]\n",
    "else:\n",
    "    n_extra= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    \n",
    "    def __init__( self, pos_net, neg_net, w, b, batch_size, num_unrollings, drop_prob_list, name=''):\n",
    "    \n",
    "        # 1. Create placeholders for inputs \n",
    "        self.X = list()\n",
    "        for iaux in range(num_unrollings + 1):\n",
    "            self.X.append(tf.placeholder(tf.float32, shape=[batch_size, vc.vocabulary_size], \n",
    "                                             name= name+'X_i'+str(iaux)+'_All'))\n",
    "        aux_inputs = self.X[:num_unrollings]\n",
    "        aux_labels = self.X[1:]  # inputs shifted by one time step.\n",
    "\n",
    "        # Create a list for store the placeholders for the labels\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "       \n",
    "\n",
    "        # -------------------- unrolling of the network --------------------------- # \n",
    "        self.pos_unroll, _= pos_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   )\n",
    "\n",
    "        self.neg_unroll, _= neg_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        # error_per_sample is a vector, its shape is changed to have each unrolling in separate columns\n",
    "        output_pos= tf.reshape(self.pos_unroll.error_per_sample,[num_unrollings,batch_size])  \n",
    "        output_neg= tf.reshape(self.neg_unroll.error_per_sample,[num_unrollings,batch_size]) \n",
    "\n",
    "        output_pos_mean= tf.reduce_mean(output_pos, reduction_indices= 0) \n",
    "        output_neg_mean= tf.reduce_mean(output_neg, reduction_indices= 0) \n",
    "\n",
    "        output_mean= tf.transpose(tf.pack( [ output_pos_mean, output_neg_mean ]))\n",
    "\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "\n",
    "\n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        \n",
    "        # prediction error\n",
    "        #Lp = tf.reduce_mean( tf.mul(self.labels, output_pos_mean) + tf.mul(self.labels-1, output_neg_mean))\n",
    "        Lp = tf.reduce_mean( tf.mul(tf.squeeze(self.labels), output_pos_mean) + \n",
    "                             tf.mul(tf.squeeze(1-self.labels), output_neg_mean)\n",
    "                           )\n",
    "        # counter-prediction penalty\n",
    "        #\n",
    "        Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.exp(-output_pos_mean)) + \n",
    "                              tf.mul(tf.squeeze(self.labels), tf.exp(-output_neg_mean))\n",
    "                            )\n",
    "        \n",
    "        #Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.reduce_mean(tf.exp(-output_pos), reduction_indices= 0) ) + \n",
    "        #                      tf.mul(tf.squeeze(self.labels), tf.reduce_mean(tf.exp(-output_neg), reduction_indices= 0) )\n",
    "        #                    )\n",
    "        \n",
    "        # regularization\n",
    "        l2_c= tf.nn.l2_loss(w)\n",
    "        \n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp + beta*(1.0/Lcp)\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp - beta*(Lcp)\n",
    "        self.alpha_r = tf.placeholder(tf.float32)\n",
    "        self.beta_r = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.loss = tf.reduce_mean( self.error_per_sample ) + self.alpha_r*Lp + self.beta_r*Lcp + lambda_w*l2_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # For dropout\n",
    "    drop_prob = tf.placeholder(tf.float32)\n",
    "    drop_prob_list = [ drop_prob for i in range(len(num_nodes)+1)]\n",
    "    drop_prob_list[0]= None\n",
    "     \n",
    "    # 1. Define positive and negative neural networks\n",
    "    pos_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= pos_net_name)\n",
    "        \n",
    "    neg_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= neg_net_name)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    #w = tf.Variable(tf.truncated_normal([2, 1], -0.1, 0.1), name=('w_class')) # unconstrained w\n",
    "    #w = tf.Variable(tf.constant( [[-2.0], [2.0]]), name=('w_class'), trainable=False) # fixed w\n",
    "    \n",
    "    #w = tf.Variable(tf.truncated_normal([1, 1], 0.3, 0.4), name=('w_class')) # unconstrained w\n",
    "    w = tf.Variable(tf.constant( [[0.3]] ), name=('w_class')) # fixed w\n",
    "    w = tf.concat(0, [-w, w])\n",
    "    \n",
    "    b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Define unrolling for training\n",
    "    train= ModelSetup( pos_net, neg_net, w, b, batch_size, num_unrollings, \n",
    "                       drop_prob_list, \n",
    "                       name='train_')\n",
    "       \n",
    "    # 3. Define unrolling for validation\n",
    "    if allow_valid:\n",
    "        valid= ModelSetup( pos_net, neg_net, w, b, batch_size_val, num_unrollings_val, \n",
    "                           drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                           name='valid_')\n",
    "\n",
    "    # 4. Define unrolling for testing\n",
    "    pos_gen_test, _= pos_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    neg_gen_test, _= neg_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    \n",
    "          \n",
    "    \n",
    "    # 5. Define optimizer    \n",
    "    # 1. specify the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) # ADAM 0.001\n",
    "    \n",
    "    # 2. get the gradients and variables\n",
    "    # grads_and_vars is a list of tuples (gradient, variable). \n",
    "    grads_and_vars = optimizer.compute_gradients(train.loss) \n",
    "    gradients, v = zip(*grads_and_vars)\n",
    "    \n",
    "    # 3. process the gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, grad_clip_thresh)  #1.25 #0.025 #0.001(last used)\n",
    "    # 4. apply the gradients to the optimization procedure\n",
    "    optimizer = optimizer.apply_gradients( zip(gradients, v) ) # ADAM\n",
    "    \n",
    "    # for prediction\n",
    "    train_pred = tf.nn.sigmoid(train.logits)\n",
    "    \n",
    "    # Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Energy predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_pos= False\n",
    "load_pos_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_pos.ckpt\"\n",
    "    \n",
    "reload_neg= False\n",
    "load_neg_file= working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_neg.ckpt\"\n",
    "\n",
    "reload_all= True\n",
    "load_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run-1) +\"r_All\"+comment+\".ckpt\";\n",
    "\n",
    "save_all= True\n",
    "save_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run) +\"r_All\"+comment+\".ckpt\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_run==1:\n",
    "    reload_all= False;\n",
    "else:\n",
    "    reload_pos= False;\n",
    "    reload_neg= False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Initialized\n",
      "step | Train_err, Train_mis | Valid_err, Valid_mis | loss\n",
      "0 | 0.000000 (0.450000), 0 | 0.485600, 48 | 81.811356 | w:[-0.295006,0.295006], b:0.000000\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000 \n",
    "summary_frequency = 50\n",
    "n_valid_tests = 50\n",
    "n_characters_step= num_unrollings*batch_size\n",
    "\n",
    "aux_print=0\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # -------------------------------- Load weigths from file, or initialize variables ------------------------------------\n",
    "    if reload_all:\n",
    "        saver.restore(session, load_all_file)\n",
    "        #session.run( global_step.assign(0) ) # SGD\n",
    "        \n",
    "    elif (reload_pos and reload_neg):\n",
    "        tf.initialize_all_variables().run()\n",
    "        pos_net.saver.restore(session, load_pos_file)\n",
    "        neg_net.saver.restore(session, load_neg_file)\n",
    "        print('Weights for positive and negative networks loaded')\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Weights Initialized')\n",
    "    \n",
    "    # ------------------------------------------- Training loop --------------------------------------------------\n",
    "    print('step | Train_err, Train_mis | Valid_err, Valid_mis | loss')\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    mean_mis_pred = 0.0\n",
    "    for step in range(num_steps):\n",
    "        # 1. Get next batch\n",
    "        batch_X, batch_y = dataset.train.next_batch()\n",
    "        \n",
    "        # 2. Setup feed dictionary for network 1 and network 2\n",
    "        feed_dict = dict()\n",
    "        # For dropout\n",
    "        feed_dict[drop_prob] = dropout_cons\n",
    "        # hyperparameters\n",
    "        #if step<200:\n",
    "        #    feed_dict[train.alpha_r] = 1.0\n",
    "        #    feed_dict[train.beta_r] = 0.0\n",
    "        #else:\n",
    "        feed_dict[train.alpha_r] = alpha\n",
    "        feed_dict[train.beta_r] = beta\n",
    "            \n",
    "        # Inroduce labels\n",
    "        feed_dict[train.labels] = batch_y\n",
    "        # Introduce inputs\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train.X[i]] = batch_X[i]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 3 Run optimizer\n",
    "        #_, l, lr = session.run( [optimizer, loss_train, learning_rate], feed_dict=feed_dict) # SGD\n",
    "        _, l, pred_train_aux = session.run( [optimizer, train.loss, train_pred], feed_dict=feed_dict) # ADAM\n",
    "        \n",
    "        mean_mis_pred += np.sum( np.not_equal( np.greater(pred_train_aux, 0.5), np.greater(batch_y, 0.5)) )\n",
    "        mean_loss += l\n",
    "        \n",
    "        # --------------------------------------------- logging ------------------------------------------------\n",
    "        if dataset.train.batch_generators[0]._text_size<aux_print :\n",
    "            print('+'*80)\n",
    "            print('')\n",
    "            aux_print = 0\n",
    "        else:\n",
    "            aux_print += n_characters_step\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                mean_mis_pred = mean_mis_pred / summary_frequency\n",
    "            \n",
    "            '''train mis classification'''        \n",
    "            prediction_train = session.run( [tf.nn.sigmoid(train.logits)], feed_dict=feed_dict)          \n",
    "            mis_pred_train = np.sum( np.not_equal( np.greater(prediction_train, 0.5), np.greater(batch_y, 0.5)) )\n",
    "            \n",
    "            # ---------- Print loss in validation dataset ---------\n",
    "            if allow_valid:\n",
    "                '''valid mis classification'''\n",
    "                mis_pred_val = 0\n",
    "                for step_valid in range(n_valid_tests):\n",
    "                    # 1. Get next batch\n",
    "                    batch_X_val, batch_y_val = dataset.valid.next_batch()        \n",
    "                    feed_dict_val = dict()\n",
    "                    # For dropout\n",
    "                    feed_dict_val[drop_prob] = 1.0\n",
    "                    # Inroduce labels\n",
    "                    feed_dict_val[valid.labels] = batch_y_val\n",
    "                    # Introduce inputs\n",
    "                    for i in range(num_unrollings_val+1):\n",
    "                        feed_dict_val[valid.X[i]] = batch_X_val[i]\n",
    "                    # 2. Get prediction\n",
    "                    prediction_val = session.run( [tf.nn.sigmoid(valid.logits)], feed_dict=feed_dict_val)          \n",
    "                    mis_pred_val = mis_pred_val*step_valid\n",
    "                    mis_pred_val += np.sum( np.not_equal( np.greater(prediction_val, 0.5), np.greater(batch_y_val, 0.5)) )\n",
    "                    mis_pred_val = mis_pred_val/(step_valid+1)\n",
    "                    #Delete from here\n",
    "                    #print('err=', np.sum( np.not_equal( np.greater(prediction_val, 0.5), np.greater(batch_y_val, 0.5)) ))\n",
    "                    #to here\n",
    "                    \n",
    "                    #if ((mis_pred_val/batch_y_val.shape[0])>0.2) and (step_valid==0):\n",
    "                    #    break                        \n",
    "                    #elif (step_valid==0):\n",
    "                    #    break\n",
    "                    #    print('Low validation error reached:', (mis_pred_val/batch_y_val.shape[0]))\n",
    "                \n",
    "                ''' weights and bias for classficator '''\n",
    "                # print values for b and w\n",
    "                b_t= b.eval()\n",
    "                w_t= w.eval()\n",
    "                \n",
    "                print('%d | %f (%f), %d | %f, %d | %f | w:[%f,%f], b:%f' % (step, \n",
    "                                                     mis_pred_train/batch_y.shape[0], mean_mis_pred/batch_y.shape[0], mis_pred_train,\n",
    "                                                     mis_pred_val/batch_y_val.shape[0], mis_pred_val,\n",
    "                                                     mean_loss,\n",
    "                                                     w_t[0], w_t[1], b_t[0]\n",
    "                                                    ))    \n",
    "                #if(mis_pred_val/batch_y_val.shape[0] < 0.20):\n",
    "                #    break\n",
    "            else:\n",
    "                print('%d | %f, %f | %f' % (step, mis_pred_train/batch_y.shape[0], mis_pred_train, mean_loss))\n",
    "                      \n",
    "            mean_loss = 0\n",
    "            mean_mis_pred = 0\n",
    "            \n",
    "    if save_all:\n",
    "        save_path = saver.save(session, save_all_file)\n",
    "        print(\"LSTM cell Model for negative reviews saved in file: %s\" % save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some samples from positive and negative networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Load weigths from file, or initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(session, save_all_file)\n",
    "    print('Weights loaded')\n",
    "        \n",
    "    # print values for b and w\n",
    "    b_t= b.eval()\n",
    "    w_t= w.eval()\n",
    "    #b_t= session.run(b)\n",
    "    print('b=', b_t)\n",
    "    print('w=', w_t)\n",
    "    \n",
    "    \n",
    "    # For positive network\n",
    "    print('a' + '-'*30 + 'pos' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        pos_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(pos_gen_test.y).eval({pos_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)\n",
    "    \n",
    "    # For negative network\n",
    "    print('a' + '-'*30 + 'neg' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        neg_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(neg_gen_test.y).eval({neg_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
