{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous generation-classification using LSTM (only classification)\n",
    "Implementation of the paper: Simultaneous Generation-classification using lstm \n",
    "\n",
    "Authors: Daniel L. Marino, Kasun Amarasinghe, Milos Manic\n",
    "\n",
    "This script implements the training using only classification as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#************************************************************************\n",
    "#      __   __  _    _  _____   _____\n",
    "#     /  | /  || |  | ||     \\ /  ___|\n",
    "#    /   |/   || |__| ||    _||  |  _\n",
    "#   / /|   /| ||  __  || |\\ \\ |  |_| |\n",
    "#  /_/ |_ / |_||_|  |_||_| \\_\\|______|\n",
    "#\n",
    "#   Copyright (2016) Modern Heuristics Research Group (MHRG)\n",
    "#   Virginia Commonwealth University (VCU), Richmond, VA\n",
    "#   http://www.people.vcu.edu/~mmanic/\n",
    "#   \n",
    "#   This program is free software: you can redistribute it and/or modify\n",
    "#   it under the terms of the GNU General Public License as published by\n",
    "#   the Free Software Foundation, either version 3 of the License, or\n",
    "#   (at your option) any later version.\n",
    "#\n",
    "#   This program is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#   GNU General Public License for more details.\n",
    "#  \n",
    "#   Any opinions, findings, and conclusions or recommendations expressed \n",
    "#   in this material are those of the author's(s') and do not necessarily \n",
    "#   reflect the views of any other entity.\n",
    "#  \n",
    "#   ***********************************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "\n",
    "from twodlearn.tf_lib.Feedforward import LinearLayer\n",
    "from twodlearn.tf_lib.Recurrent import *\n",
    "\n",
    "import sys\n",
    "working_dir= os.getcwd()\n",
    "\n",
    "print('Working directory:', working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allow_valid= True\n",
    "allow_test= True\n",
    "Valid_Percentage= 0.1 # i.e. 10% of the data will be used for validation\n",
    "\n",
    "pos_net_name= '_pos'\n",
    "neg_net_name= '_neg'\n",
    "\n",
    "activation_function='tanh'\n",
    "num_nodes = [50, 50] #num_nodes: Nodes for the LSTM cell\n",
    "alpha = 10.0 #0.1 #0.1\n",
    "beta = 0.1 #0.01 #10000.01\n",
    "lambda_w = 0.00001\n",
    "\n",
    "dropout_cons = 0.8\n",
    "\n",
    "Allow_Bias= False \n",
    "\n",
    "learning_rate= 0.005      # 0.001\n",
    "grad_clip_thresh= 1.1       # 0.00001\n",
    "\n",
    "current_run= 1\n",
    "batch_size= 64 #64\n",
    "num_unrollings= 64 #100\n",
    "\n",
    "batch_size_val=64 #len(valid_text_pos)/num_unrollings_val #500\n",
    "num_unrollings_val= num_unrollings #100\n",
    "\n",
    "batch_size_test= 64 \n",
    "num_unrollings_test= num_unrollings #100\n",
    "\n",
    "comment='_noDropout_Lc_64unrol_standarloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: [50, 50]\n",
      "alpha: 10.0 , beta: 0.1 , lambda_w: 1e-05\n",
      "num_unrollings: 64 , batch_size: 64 , batch_size_val: 64\n",
      "learning_rate: 0.005 grad_clip_thresh: 1.1\n"
     ]
    }
   ],
   "source": [
    "model_version = 'L'+str(len(num_nodes))\n",
    "print(\"num_nodes:\",num_nodes)\n",
    "print(\"alpha:\",alpha, \", beta:\",beta,\", lambda_w:\", lambda_w)\n",
    "print(\"num_unrollings:\",num_unrollings, \", batch_size:\",batch_size,\", batch_size_val:\", batch_size_val)\n",
    "print(\"learning_rate:\", learning_rate, 'grad_clip_thresh:', grad_clip_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "(64, 1)\n",
      "(64, 4001)\n",
      "put togeth that you simpli have to some kind of fond for it half of the film at least revolv on the wacki adventur of and these two local own and run a boat rent shop near the lake but spend most of their day pick their nose and over fascin stuff like to spell the word it is mostli dure their laurel hardi situat \n",
      "[ 0.]\n",
      "factor that can tire the audienc due to lack of time to all of the action but it is in the end a detail homag to a great citi and suppli the viewer with mani to re visit like a of a time in pari it is a film worth see multipl time some movi seem to be made befor we are readi for them \n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "vc= pickle.load( open( \"imdb_vc.pkl\", \"rb\" ) )\n",
    "num_inputs=  vc.vocabulary_size\n",
    "num_outputs= vc.vocabulary_size\n",
    "\n",
    "dataset= pickle.load( open( \"imdb_dataset.pkl\", \"rb\" ) )\n",
    "\n",
    "# set batch_size and number of unrolligns\n",
    "dataset.train.set_batch_and_unrollings(batch_size, num_unrollings)\n",
    "dataset.valid.set_batch_and_unrollings(batch_size_val, num_unrollings_val)\n",
    "dataset.test.set_batch_and_unrollings(batch_size_test, num_unrollings_test)\n",
    "\n",
    "# print a sample of the dataset\n",
    "train_x, train_y= dataset.train.next_batch()\n",
    "print(len(train_x))\n",
    "print(train_y.shape)\n",
    "print(train_x[0].shape)\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][0,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[0])\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][50,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[50])\n",
    "\n",
    "valid_x, valid_y= dataset.valid.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs: 4001\n",
      "pos train length: 2467784\n",
      "neg train length: 2467784\n",
      "pos valid length: 274198\n",
      "neg valid length: 274198\n",
      "pos test length: 2686382\n",
      "neg test length: 2686382\n"
     ]
    }
   ],
   "source": [
    "print('# inputs:',num_inputs)\n",
    "\n",
    "print('pos train length:',dataset.train.batch_generators[0]._text_size )\n",
    "print('neg train length:',dataset.train.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos valid length:',dataset.valid.batch_generators[0]._text_size )\n",
    "print('neg valid length:',dataset.valid.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos test length:',dataset.test.batch_generators[0]._text_size )\n",
    "print('neg test length:',dataset.test.batch_generators[1]._text_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_error(predictions, labels):\n",
    "    \"\"\" number of samples wrongly classified  \"\"\"\n",
    "    return np.sum( np.not_equal( np.greater(predictions, 0.5), \n",
    "                                 np.greater(labels, 0.5)) )/labels.shape[0]\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10 # this is to prevent that log() returns minus infinity\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vc.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vc.vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myLstmNet(LstmNet):\n",
    "    def get_extra_inputs(self, i, h_list, state_list):\n",
    "        #print('OK:', len(h_list))\n",
    "        return i\n",
    "    \n",
    "    def evaluate_final_output(self, outputs_list, inputs_list, h_list ):\n",
    "        ''' Calculates the final output of the neural network, usually it is just a linear transformation\n",
    "            - outputs_list: list with the outputs from the last lstm cell\n",
    "            - inputs_list: list of inputs to the network\n",
    "            - h_list: list with all hidden outputs from all the cells\n",
    "        '''\n",
    "        all_hidden = list()\n",
    "        \n",
    "        for t in h_list: # go trough each time step\n",
    "            all_hidden.append( tf.concat(1,t) )\n",
    "        return self.out_layer.evaluate(tf.concat(0, all_hidden))  \n",
    "            \n",
    "    \n",
    "if len(num_nodes)>1:\n",
    "    n_extra= [num_inputs for i in range(len(num_nodes)+1)]\n",
    "    n_extra[0]= 0\n",
    "    n_extra[-1]= sum(num_nodes) - num_nodes[-1]\n",
    "else:\n",
    "    n_extra= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    \n",
    "    def __init__( self, pos_net, neg_net, w, b, batch_size, num_unrollings, drop_prob_list, name=''):\n",
    "    \n",
    "        # 1. Create placeholders for inputs \n",
    "        self.X = list()\n",
    "        for iaux in range(num_unrollings + 1):\n",
    "            self.X.append(tf.placeholder(tf.float32, shape=[batch_size, vc.vocabulary_size], \n",
    "                                             name= name+'X_i'+str(iaux)+'_All'))\n",
    "        aux_inputs = self.X[:num_unrollings]\n",
    "        aux_labels = self.X[1:]  # inputs shifted by one time step.\n",
    "\n",
    "        # Create a list for store the placeholders for the labels\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "       \n",
    "\n",
    "        # -------------------- unrolling of the network --------------------------- # \n",
    "        self.pos_unroll, _= pos_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   )\n",
    "\n",
    "        self.neg_unroll, _= neg_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        # error_per_sample is a vector, its shape is changed to have each unrolling in separate columns\n",
    "        y_pos= tf.reshape(self.pos_unroll.y, [-1, batch_size, num_outputs])\n",
    "        y_neg= tf.reshape(self.neg_unroll.y, [-1, batch_size, num_outputs])\n",
    "        \n",
    "        y_pos_mean= tf.reduce_mean(y_pos, reduction_indices=0)\n",
    "        y_neg_mean= tf.reduce_mean(y_neg, reduction_indices=0)\n",
    "        \n",
    "        output_mean= tf.concat(1, [ y_pos_mean, y_neg_mean ])\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "        \n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        \n",
    "        '''\n",
    "        output_pos= tf.reshape(self.pos_unroll.error_per_sample,[num_unrollings,batch_size])  \n",
    "        output_neg= tf.reshape(self.neg_unroll.error_per_sample,[num_unrollings,batch_size]) \n",
    "\n",
    "        output_pos_mean= tf.reduce_mean(output_pos, reduction_indices= 0) \n",
    "        output_neg_mean= tf.reduce_mean(output_neg, reduction_indices= 0) \n",
    "\n",
    "        output_mean= tf.transpose(tf.pack( [ output_pos_mean, output_neg_mean ]))\n",
    "\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "\n",
    "\n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        '''\n",
    "        \n",
    "        # regularization\n",
    "        l2_c= tf.nn.l2_loss(w)\n",
    "        \n",
    "        # loss\n",
    "        self.loss = tf.reduce_mean( self.error_per_sample ) \n",
    "        \n",
    "        # classification error\n",
    "        self.error = tf.reduce_mean( tf.to_float(tf.not_equal( tf.greater(tf.nn.sigmoid(self.logits), 0.5), \n",
    "                                                               tf.greater(self.labels, 0.5))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # For dropout\n",
    "    drop_prob = tf.placeholder(tf.float32)\n",
    "    drop_prob_list = [ drop_prob for i in range(len(num_nodes)+1)]\n",
    "    drop_prob_list[0]= None\n",
    "     \n",
    "    # 1. Define positive and negative neural networks\n",
    "    pos_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= pos_net_name)\n",
    "        \n",
    "    neg_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= neg_net_name)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_outputs*2, 1], -0.1, 0.1), name=('w_class')) # unconstrained w\n",
    "    b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    #w = tf.Variable(tf.constant( [[0.3]] ), name=('w_class')) # fixed w\n",
    "    #w = tf.concat(0, [-w, w])\n",
    "    #b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Define unrolling for training\n",
    "    train= ModelSetup( pos_net, neg_net, w, b, batch_size, num_unrollings, \n",
    "                       drop_prob_list, \n",
    "                       name='train_')\n",
    "       \n",
    "    # 3. Define unrolling for validation\n",
    "    if allow_valid:\n",
    "        valid= ModelSetup( pos_net, neg_net, w, b, batch_size_val, num_unrollings_val, \n",
    "                           drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                           name='valid_')\n",
    "        \n",
    "        \n",
    "    # 4. Define unrolling for testing\n",
    "    if allow_test:\n",
    "        test= ModelSetup( pos_net, neg_net, w, b, batch_size_test, num_unrollings_test, \n",
    "                          drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                          name='test_')\n",
    "    \n",
    "    # 5. Define unrolling for testing generator\n",
    "    pos_gen_test, _= pos_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    neg_gen_test, _= neg_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    \n",
    "          \n",
    "    \n",
    "    # 6. Define optimizer    \n",
    "    # 1. specify the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) # ADAM 0.001\n",
    "    \n",
    "    # 2. get the gradients and variables\n",
    "    # grads_and_vars is a list of tuples (gradient, variable). \n",
    "    grads_and_vars = optimizer.compute_gradients(train.loss) \n",
    "    gradients, v = zip(*grads_and_vars)\n",
    "    \n",
    "    # 3. process the gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, grad_clip_thresh)  #1.25 #0.025 #0.001(last used)\n",
    "    # 4. apply the gradients to the optimization procedure\n",
    "    optimizer = optimizer.apply_gradients( zip(gradients, v) ) # ADAM\n",
    "    \n",
    "    # for prediction\n",
    "    train_pred = tf.nn.sigmoid(train.logits)\n",
    "    \n",
    "    # Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Energy predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_pos= False\n",
    "load_pos_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_pos.ckpt\"\n",
    "    \n",
    "reload_neg= False\n",
    "load_neg_file= working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_neg.ckpt\"\n",
    "\n",
    "reload_all= True\n",
    "load_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run-1) +\"r_All\"+comment+\".ckpt\";\n",
    "\n",
    "save_all= True\n",
    "save_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run) +\"r_All\"+comment+\".ckpt\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_run==1:\n",
    "    reload_all= False;\n",
    "else:\n",
    "    reload_pos= False;\n",
    "    reload_neg= False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Initialized\n",
      "step | Train_err | Valid_err | loss\n",
      "0 | 0.500000 | 0.500000 | 1.051771 \n",
      "50 | 0.450937 | 0.283437 | 1.306119 \n",
      "100 | 0.332813 | 0.258750 | 0.678269 \n",
      "150 | 0.268750 | 0.333438 | 0.559123 \n",
      "200 | 0.254688 | 0.376563 | 0.538852 \n",
      "250 | 0.291250 | 0.236875 | 0.602461 \n",
      "300 | 0.265000 | 0.217500 | 0.528132 \n",
      "350 | 0.249688 | 0.229687 | 0.527801 \n",
      "400 | 0.237500 | 0.229063 | 0.491978 \n",
      "450 | 0.225938 | 0.293125 | 0.485880 \n",
      "500 | 0.231875 | 0.225000 | 0.488567 \n",
      "550 | 0.247188 | 0.224062 | 0.532680 \n",
      "600 | 0.238750 | 0.205313 | 0.489500 | saved\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "650 | 0.234063 | 0.280938 | 0.504992 \n",
      "700 | 0.239063 | 0.263125 | 0.522179 \n",
      "750 | 0.218750 | 0.224062 | 0.460025 \n",
      "800 | 0.217500 | 0.210938 | 0.452480 \n",
      "850 | 0.237500 | 0.253437 | 0.485278 \n",
      "900 | 0.209687 | 0.214688 | 0.458009 \n",
      "950 | 0.199687 | 0.192188 | 0.436176 | saved\n",
      "1000 | 0.210938 | 0.207500 | 0.454548 \n",
      "1050 | 0.224688 | 0.228750 | 0.474958 \n",
      "1100 | 0.204375 | 0.222188 | 0.436730 \n",
      "1150 | 0.205313 | 0.206875 | 0.440205 \n",
      "1200 | 0.197813 | 0.187500 | 0.431266 | saved\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1250 | 0.177500 | 0.215938 | 0.387835 \n",
      "1300 | 0.170313 | 0.247812 | 0.394938 \n",
      "1350 | 0.153125 | 0.242500 | 0.360438 \n",
      "1400 | 0.169375 | 0.228437 | 0.386699 \n",
      "1450 | 0.195000 | 0.229687 | 0.443853 \n",
      "1500 | 0.161875 | 0.196563 | 0.382809 \n",
      "1550 | 0.170000 | 0.200313 | 0.384867 \n",
      "1600 | 0.135000 | 0.262188 | 0.332795 \n",
      "1650 | 0.122188 | 0.212188 | 0.306722 \n",
      "1700 | 0.153438 | 0.225938 | 0.367904 \n",
      "1750 | 0.166250 | 0.219375 | 0.371163 \n",
      "1800 | 0.163750 | 0.206250 | 0.394691 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1850 | 0.155312 | 0.261875 | 0.362229 \n",
      "1900 | 0.168750 | 0.203750 | 0.375184 \n",
      "1950 | 0.152812 | 0.206875 | 0.337153 \n",
      "2000 | 0.147187 | 0.244688 | 0.338997 \n",
      "2050 | 0.162812 | 0.256563 | 0.371300 \n",
      "2100 | 0.162188 | 0.208437 | 0.367552 \n",
      "2150 | 0.133437 | 0.189375 | 0.322137 \n",
      "2200 | 0.134063 | 0.214375 | 0.312912 \n",
      "2250 | 0.136250 | 0.247500 | 0.327117 \n",
      "2300 | 0.133437 | 0.211250 | 0.314340 \n",
      "2350 | 0.145000 | 0.237813 | 0.333571 \n",
      "2400 | 0.148125 | 0.217812 | 0.338508 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "2450 | 0.120313 | 0.225625 | 0.283565 \n",
      "2500 | 0.109375 | 0.275625 | 0.254559 \n",
      "2550 | 0.113750 | 0.210938 | 0.336731 \n",
      "2600 | 0.143750 | 0.220625 | 0.356247 \n",
      "2650 | 0.121875 | 0.240000 | 0.327900 \n",
      "2700 | 0.100000 | 0.257188 | 0.266249 \n",
      "2750 | 0.117813 | 0.240937 | 0.302847 \n",
      "2800 | 0.124063 | 0.244688 | 0.310067 \n",
      "2850 | 0.093750 | 0.226562 | 0.263848 \n",
      "2900 | 0.139687 | 0.204687 | 0.351051 \n",
      "2950 | 0.109375 | 0.263438 | 0.289157 \n",
      "Learning finished, weights saved in file: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb/weights/Weights_LSTM_L2_50u_1r_All_noDropout_Lc_64unrol_standarloss.ckpt\n",
      "Classification error on test dataset: 0.2484375\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3000 \n",
    "summary_frequency = 50\n",
    "n_valid_tests = 50\n",
    "n_tests = 50\n",
    "n_characters_step= num_unrollings*batch_size\n",
    "\n",
    "train_error_l= list()\n",
    "valid_error_l= list()\n",
    "\n",
    "aux_print=0\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # -------------------------------- Load weigths from file, or initialize variables ------------------------------------\n",
    "    if reload_all:\n",
    "        saver.restore(session, load_all_file)\n",
    "        #session.run( global_step.assign(0) ) # SGD\n",
    "        \n",
    "    elif (reload_pos and reload_neg):\n",
    "        tf.initialize_all_variables().run()\n",
    "        pos_net.saver.restore(session, load_pos_file)\n",
    "        neg_net.saver.restore(session, load_neg_file)\n",
    "        print('Weights for positive and negative networks loaded')\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Weights Initialized')\n",
    "    \n",
    "    # ------------------------------------------- Training loop --------------------------------------------------\n",
    "    print('step | Train_err | Valid_err | loss')\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    mean_error = 0.0\n",
    "    for step in range(num_steps):\n",
    "        # 1. Get next batch\n",
    "        batch_X, batch_y = dataset.train.next_batch()\n",
    "        \n",
    "        # 2. Setup feed dictionary for network 1 and network 2\n",
    "        feed_dict = dict()\n",
    "        # For dropout\n",
    "        feed_dict[drop_prob] = dropout_cons\n",
    "        # hyperparameters\n",
    "        #feed_dict[train.alpha_r] = alpha\n",
    "        #feed_dict[train.beta_r] = beta\n",
    "            \n",
    "        # Inroduce labels\n",
    "        feed_dict[train.labels] = batch_y\n",
    "        # Introduce inputs\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train.X[i]] = batch_X[i]\n",
    "              \n",
    "        # 3 Run optimizer\n",
    "        #_, l, lr = session.run( [optimizer, loss_train, learning_rate], feed_dict=feed_dict) # SGD\n",
    "        _, l, train_error = session.run( [optimizer, train.loss, train.error], feed_dict=feed_dict) # ADAM\n",
    "        \n",
    "        mean_loss += l\n",
    "        mean_error += train_error #classification_error(pred_train_aux, batch_y) \n",
    "        \n",
    "        # --------------------------------------------- logging ------------------------------------------------\n",
    "        if dataset.train.batch_generators[0]._text_size<aux_print :\n",
    "            print('+'*80)\n",
    "            print('')\n",
    "            aux_print = 0\n",
    "        else:\n",
    "            aux_print += n_characters_step\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                mean_error = mean_error / summary_frequency\n",
    "                        \n",
    "            # ---------- Print loss in validation dataset ---------\n",
    "            if allow_valid:\n",
    "                ''' classification error on validation dataset '''\n",
    "                mean_error_val = 0.0\n",
    "                for step_valid in range(n_valid_tests):\n",
    "                    # 1. Get next batch\n",
    "                    batch_X_val, batch_y_val = dataset.valid.next_batch()        \n",
    "                    feed_dict_val = dict()\n",
    "                    # For dropout\n",
    "                    feed_dict_val[drop_prob] = 1.0\n",
    "                    # Inroduce labels\n",
    "                    feed_dict_val[valid.labels] = batch_y_val\n",
    "                    # Introduce inputs\n",
    "                    for i in range(num_unrollings_val+1):\n",
    "                        feed_dict_val[valid.X[i]] = batch_X_val[i]\n",
    "                    # 2. Get classification error\n",
    "                    #prediction_val = session.run( [tf.nn.sigmoid(valid.logits)], feed_dict=feed_dict_val)          \n",
    "                    #mean_error_val = mean_error_val*step_valid\n",
    "                    #mean_error_val += classification_error(prediction_val, batch_y_val)\n",
    "                    #mean_error_val = mean_error_val/(step_valid+1)\n",
    "                    [valid_error] = session.run( [valid.error], feed_dict=feed_dict_val)          \n",
    "                    mean_error_val = mean_error_val*step_valid\n",
    "                    mean_error_val += valid_error\n",
    "                    mean_error_val = mean_error_val/(step_valid+1)\n",
    "                                    \n",
    "                ''' print information '''\n",
    "                train_error_l.append(mean_error)\n",
    "                valid_error_l.append(mean_error_val)\n",
    "                \n",
    "                if save_all and valid_error_l[-1] == min(valid_error_l) and step>200:\n",
    "                    save_path = saver.save(session, save_all_file)\n",
    "                    print('%d | %f | %f | %f | saved' % (step, \n",
    "                                                         mean_error, \n",
    "                                                         mean_error_val, \n",
    "                                                         mean_loss\n",
    "                                                        ))    \n",
    "                \n",
    "                else:\n",
    "                    print('%d | %f | %f | %f ' % (step, \n",
    "                                                  mean_error, \n",
    "                                                  mean_error_val, \n",
    "                                                  mean_loss\n",
    "                                                 ))     \n",
    "                \n",
    "                #if(mean_error_val < 0.19):\n",
    "                #    break\n",
    "            else:\n",
    "                train_error_l.append(mean_error)\n",
    "                print('%d | %f | %f' % (step, mean_error, mean_loss))\n",
    "                      \n",
    "            mean_loss = 0.0\n",
    "            mean_error = 0.0\n",
    "            \n",
    "    if save_all and allow_valid:\n",
    "        print(\"Learning finished, weights saved in file: %s\" % save_path)\n",
    "    \n",
    "    \n",
    "    ''' classification error on test dataset '''        \n",
    "    if allow_test:\n",
    "        mean_error_test = 0\n",
    "        for step_test in range(n_tests):\n",
    "            # 1. Get next batch\n",
    "            batch_X_test, batch_y_test = dataset.test.next_batch()        \n",
    "            feed_dict_test = dict()\n",
    "            # For dropout\n",
    "            feed_dict_test[drop_prob] = 1.0\n",
    "            # Inroduce labels\n",
    "            feed_dict_test[test.labels] = batch_y_test\n",
    "            # Introduce inputs\n",
    "            for i in range(num_unrollings_test+1):\n",
    "                feed_dict_test[test.X[i]] = batch_X_test[i]\n",
    "            # 2. Get prediction\n",
    "            prediction_test = session.run( [tf.nn.sigmoid(test.logits)], feed_dict=feed_dict_test)          \n",
    "            mean_error_test = mean_error_test*step_test\n",
    "            mean_error_test += classification_error(prediction_test, batch_y_test)\n",
    "            mean_error_test = mean_error_test/(step_test+1)\n",
    "            \n",
    "        print(\"Classification error on test dataset:\", mean_error_test)\n",
    "        \n",
    "    ''' perplexity '''\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6d64db1048>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6d64db1828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "train_error_a = np.array(train_error_l)\n",
    "valid_error_a = np.array(valid_error_l)\n",
    "\n",
    "plt.plot(np.linspace(0, train_error_a.shape[0], train_error_a.shape[0]) , train_error_a )\n",
    "plt.plot(np.linspace(0, valid_error_a.shape[0], train_error_a.shape[0]) , valid_error_a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some samples from positive and negative networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n",
      "b= [ 0.]\n",
      "w= [[-0.04025425]\n",
      " [-0.01381715]\n",
      " [ 0.02272164]\n",
      " ..., \n",
      " [-0.03181576]\n",
      " [-0.00290817]\n",
      " [-0.00982929]]\n",
      "a------------------------------pos------------------------------\n",
      "matur behavior gloriou balanc gradual hit beat neil shed silent coast lone anymor satir refus reminisc show fulli bu oz exampl creepi folk inde quarter eastwood did attach win bleed kurt \n",
      "spanish wrap hamilton spark player nor thick melt thank dragon extrem feet chemistri reunion mafia reunion power factori steve heal allow such craven here couldn winner compet streisand movi biggest quot \n",
      "bruce made kudo soon bodi grate rais ruthless opposit winner gal hang sexual him tip sun pretend ape speech wilson loud taylor split tarzan clark tribe function shoulder menac audienc virtual \n",
      "michael ironi pin written expert apart aw take twelv surpris cliff foot mildr excit perri anybodi maker toss site mundan fonda accompani sincer grandmoth account florida accus involv run expand louis \n",
      "disabl cole mortal rambl pass nephew loud onto combin write switch harri basi shape birth rape deepli turkey johnson eddi fiction evil johnson advanc indulg cure graduat brother comfort categori scientif \n",
      "a--------------------------------------------------------------------------------\n",
      "a------------------------------neg------------------------------\n",
      "more gori sympathet hatr lack member soon news crocodil morn kim usual canada gender lowest wrong soft tiresom reveng sue accept reunion gangster N/A sceneri wive tempt kind secur furthermor ted \n",
      "amount partner power fortun mostli absorb rifl fatal holm european camp grave faith n polanski commerci blond ya till domest jet captain colonel surviv lloyd wall bedroom disney wrestl cowboy declar \n",
      "bond treasur avail notabl awe tough sadli come mani model aka chief grace revel similar freak needless outcom nelson anticip oh lifetim station british hyde solo redempt outfit son plot cheek \n",
      "trap to care debat suppli bottom richard synopsi graduat todd mental experiment admir plot multipl presenc enhanc render those proceed instead meat hey martin error thoroughli pleasant matthau uplift topless talent \n",
      "program custom boil contract wizard hadn furi laid baker stock austen local lesli graduat victim ye hadn taught stood blade confirm medic nois insid more argument prequel unwatch food special wonder \n",
      "a--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Load weigths from file, or initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(session, save_all_file)\n",
    "    print('Weights loaded')\n",
    "        \n",
    "    # print values for b and w\n",
    "    b_t= b.eval()\n",
    "    w_t= w.eval()\n",
    "    #b_t= session.run(b)\n",
    "    print('b=', b_t)\n",
    "    print('w=', w_t)\n",
    "    \n",
    "    \n",
    "    # For positive network\n",
    "    print('a' + '-'*30 + 'pos' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        pos_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(pos_gen_test.y).eval({pos_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)\n",
    "    \n",
    "    # For negative network\n",
    "    print('a' + '-'*30 + 'neg' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        neg_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(neg_gen_test.y).eval({neg_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
