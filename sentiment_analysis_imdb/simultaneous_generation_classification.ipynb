{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous generation-classification using LSTM \n",
    "Implementation of the paper: Simultaneous Generation-classification using lstm \n",
    "\n",
    "Authors: Daniel L. Marino, Kasun Amarasinghe, Milos Manic\n",
    "\n",
    "This script implements the training using Generation and Classification as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#************************************************************************\n",
    "#      __   __  _    _  _____   _____\n",
    "#     /  | /  || |  | ||     \\ /  ___|\n",
    "#    /   |/   || |__| ||    _||  |  _\n",
    "#   / /|   /| ||  __  || |\\ \\ |  |_| |\n",
    "#  /_/ |_ / |_||_|  |_||_| \\_\\|______|\n",
    "#    \n",
    "#\n",
    "#   Copyright (2016) Modern Heuristics Research Group (MHRG)\n",
    "#   Virginia Commonwealth University (VCU), Richmond, VA\n",
    "#   http://www.people.vcu.edu/~mmanic/\n",
    "#   \n",
    "#   This program is free software: you can redistribute it and/or modify\n",
    "#   it under the terms of the GNU General Public License as published by\n",
    "#   the Free Software Foundation, either version 3 of the License, or\n",
    "#   (at your option) any later version.\n",
    "#\n",
    "#   This program is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#   GNU General Public License for more details.\n",
    "#  \n",
    "#   Any opinions, findings, and conclusions or recommendations expressed \n",
    "#   in this material are those of the author's(s') and do not necessarily \n",
    "#   reflect the views of any other entity.\n",
    "#  \n",
    "#   ***********************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "\n",
    "from twodlearn.tf_lib.Feedforward import LinearLayer\n",
    "from twodlearn.tf_lib.Recurrent import *\n",
    "\n",
    "import sys\n",
    "working_dir= os.getcwd()\n",
    "\n",
    "print('Working directory:', working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allow_valid= True\n",
    "allow_test= True\n",
    "Valid_Percentage= 0.1 # i.e. 10% of the data will be used for validation\n",
    "\n",
    "pos_net_name= '_pos'\n",
    "neg_net_name= '_neg'\n",
    "\n",
    "activation_function='tanh'\n",
    "num_nodes = [25, 25] #num_nodes: Nodes for the LSTM cell\n",
    "alpha = 10.0 #0.1 #0.1\n",
    "beta = 0.1 #0.01 #10000.01\n",
    "lambda_w = 0.00001\n",
    "\n",
    "dropout_cons = 0.8\n",
    "\n",
    "Allow_Bias= False \n",
    "\n",
    "learning_rate= 0.005      # 0.001\n",
    "grad_clip_thresh= 1.1       # 0.00001\n",
    "\n",
    "current_run= 1\n",
    "batch_size= 64 #64\n",
    "num_unrollings= 64 #100\n",
    "\n",
    "batch_size_val= 64 #len(valid_text_pos)/num_unrollings_val #500\n",
    "num_unrollings_val= num_unrollings #100\n",
    "\n",
    "batch_size_test= 64 \n",
    "num_unrollings_test= num_unrollings #100\n",
    "\n",
    "comment='_noDropout_LcLpLcp_64unrol_standarloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: [25, 25]\n",
      "alpha: 10.0 , beta: 0.1 , lambda_w: 1e-05\n",
      "num_unrollings: 64 , batch_size: 64 , batch_size_val: 64\n",
      "learning_rate: 0.005 grad_clip_thresh: 1.1\n"
     ]
    }
   ],
   "source": [
    "model_version = 'L'+str(len(num_nodes))\n",
    "print(\"num_nodes:\",num_nodes)\n",
    "print(\"alpha:\",alpha, \", beta:\",beta,\", lambda_w:\", lambda_w)\n",
    "print(\"num_unrollings:\",num_unrollings, \", batch_size:\",batch_size,\", batch_size_val:\", batch_size_val)\n",
    "print(\"learning_rate:\", learning_rate, 'grad_clip_thresh:', grad_clip_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "(64, 1)\n",
      "(64, 4001)\n",
      "put togeth that you simpli have to some kind of fond for it half of the film at least revolv on the wacki adventur of and these two local own and run a boat rent shop near the lake but spend most of their day pick their nose and over fascin stuff like to spell the word it is mostli dure their laurel hardi situat \n",
      "[ 0.]\n",
      "factor that can tire the audienc due to lack of time to all of the action but it is in the end a detail homag to a great citi and suppli the viewer with mani to re visit like a of a time in pari it is a film worth see multipl time some movi seem to be made befor we are readi for them \n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "vc= pickle.load( open( \"imdb_vc.pkl\", \"rb\" ) )\n",
    "num_inputs=  vc.vocabulary_size\n",
    "num_outputs= vc.vocabulary_size\n",
    "\n",
    "dataset= pickle.load( open( \"imdb_dataset.pkl\", \"rb\" ) )\n",
    "\n",
    "# set batch_size and number of unrolligns\n",
    "dataset.train.set_batch_and_unrollings(batch_size, num_unrollings)\n",
    "dataset.valid.set_batch_and_unrollings(batch_size_val, num_unrollings_val)\n",
    "dataset.test.set_batch_and_unrollings(batch_size_test, num_unrollings_test)\n",
    "\n",
    "# print a sample of the dataset\n",
    "train_x, train_y= dataset.train.next_batch()\n",
    "print(len(train_x))\n",
    "print(train_y.shape)\n",
    "print(train_x[0].shape)\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][0,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[0])\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][50,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[50])\n",
    "\n",
    "valid_x, valid_y= dataset.valid.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs: 4001\n",
      "pos train length: 2467784\n",
      "neg train length: 2467784\n",
      "pos valid length: 274198\n",
      "neg valid length: 274198\n",
      "pos test length: 2686382\n",
      "neg test length: 2686382\n"
     ]
    }
   ],
   "source": [
    "print('# inputs:',num_inputs)\n",
    "\n",
    "print('pos train length:',dataset.train.batch_generators[0]._text_size )\n",
    "print('neg train length:',dataset.train.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos valid length:',dataset.valid.batch_generators[0]._text_size )\n",
    "print('neg valid length:',dataset.valid.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos test length:',dataset.test.batch_generators[0]._text_size )\n",
    "print('neg test length:',dataset.test.batch_generators[1]._text_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_error(predictions, labels):\n",
    "    \"\"\" number of samples wrongly classified  \"\"\"\n",
    "    return np.sum( np.not_equal( np.greater(predictions, 0.5), \n",
    "                                 np.greater(labels, 0.5)) )/labels.shape[0]\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10 # this is to prevent that log() returns minus infinity\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vc.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vc.vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myLstmNet(LstmNet):\n",
    "    def get_extra_inputs(self, i, h_list, state_list):\n",
    "        #print('OK:', len(h_list))\n",
    "        return i\n",
    "    \n",
    "    def evaluate_final_output(self, outputs_list, inputs_list, h_list ):\n",
    "        ''' Calculates the final output of the neural network, usually it is just a linear transformation\n",
    "            - outputs_list: list with the outputs from the last lstm cell\n",
    "            - inputs_list: list of inputs to the network\n",
    "            - h_list: list with all hidden outputs from all the cells\n",
    "        '''\n",
    "        all_hidden = list()\n",
    "        \n",
    "        for t in h_list: # go trough each time step\n",
    "            all_hidden.append( tf.concat(1,t) )\n",
    "        return self.out_layer.evaluate(tf.concat(0, all_hidden))  \n",
    "            \n",
    "    \n",
    "if len(num_nodes)>1:\n",
    "    n_extra= [num_inputs for i in range(len(num_nodes)+1)]\n",
    "    n_extra[0]= 0\n",
    "    n_extra[-1]= sum(num_nodes) - num_nodes[-1]\n",
    "else:\n",
    "    n_extra= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    \n",
    "    def __init__( self, pos_net, neg_net, w, b, batch_size, num_unrollings, drop_prob_list, name=''):\n",
    "    \n",
    "        # 1. Create placeholders for inputs \n",
    "        self.X = list()\n",
    "        for iaux in range(num_unrollings + 1):\n",
    "            self.X.append(tf.placeholder(tf.float32, shape=[batch_size, vc.vocabulary_size], \n",
    "                                             name= name+'X_i'+str(iaux)+'_All'))\n",
    "        aux_inputs = self.X[:num_unrollings]\n",
    "        aux_labels = self.X[1:]  # inputs shifted by one time step.\n",
    "\n",
    "        # Create a list for store the placeholders for the labels\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "       \n",
    "\n",
    "        # -------------------- unrolling of the network --------------------------- # \n",
    "        self.pos_unroll, _= pos_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   )\n",
    "\n",
    "        self.neg_unroll, _= neg_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        # error_per_sample is a vector, its shape is changed to have each unrolling in separate columns\n",
    "        output_pos= tf.reshape(self.pos_unroll.error_per_sample,[num_unrollings,batch_size])  \n",
    "        output_neg= tf.reshape(self.neg_unroll.error_per_sample,[num_unrollings,batch_size]) \n",
    "\n",
    "        output_pos_mean= tf.reduce_mean(output_pos, reduction_indices= 0) \n",
    "        output_neg_mean= tf.reduce_mean(output_neg, reduction_indices= 0) \n",
    "\n",
    "        output_mean= tf.transpose(tf.pack( [ output_pos_mean, output_neg_mean ]))\n",
    "\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "\n",
    "\n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        \n",
    "        # prediction error\n",
    "        #Lp = tf.reduce_mean( tf.mul(self.labels, output_pos_mean) + tf.mul(self.labels-1, output_neg_mean))\n",
    "        Lp = tf.reduce_mean( tf.mul(tf.squeeze(self.labels), output_pos_mean) + \n",
    "                             tf.mul(tf.squeeze(1-self.labels), output_neg_mean)\n",
    "                           )\n",
    "        # c-p penalty\n",
    "        Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.exp(-output_pos_mean)) + \n",
    "                              tf.mul(tf.squeeze(self.labels), tf.exp(-output_neg_mean))\n",
    "                            )\n",
    "        \n",
    "        #Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.reduce_mean(tf.exp(-output_pos), reduction_indices= 0) ) + \n",
    "        #                      tf.mul(tf.squeeze(self.labels), tf.reduce_mean(tf.exp(-output_neg), reduction_indices= 0) )\n",
    "        #                    )\n",
    "        \n",
    "        # regularization\n",
    "        l2_c= tf.nn.l2_loss(w)\n",
    "        \n",
    "        # loss\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp + beta*(1.0/Lcp)\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp - beta*(Lcp)\n",
    "        self.alpha_r = tf.placeholder(tf.float32)\n",
    "        self.beta_r = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.loss = tf.reduce_mean( self.error_per_sample ) + self.alpha_r*Lp + self.beta_r*Lcp + lambda_w*l2_c\n",
    "        \n",
    "        # classification error\n",
    "        self.error = tf.reduce_mean( tf.to_float(tf.not_equal( tf.greater(tf.nn.sigmoid(self.logits), 0.5), \n",
    "                                                               tf.greater(self.labels, 0.5))) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # For dropout\n",
    "    drop_prob = tf.placeholder(tf.float32)\n",
    "    drop_prob_list = [ drop_prob for i in range(len(num_nodes)+1)]\n",
    "    drop_prob_list[0]= None\n",
    "     \n",
    "    # 1. Define positive and negative neural networks\n",
    "    pos_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= pos_net_name)\n",
    "        \n",
    "    neg_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= neg_net_name)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    #w = tf.Variable(tf.truncated_normal([2, 1], -0.1, 0.1), name=('w_class')) # unconstrained w\n",
    "    #w = tf.Variable(tf.constant( [[-2.0], [2.0]]), name=('w_class'), trainable=False) # fixed w\n",
    "    \n",
    "    #w = tf.Variable(tf.truncated_normal([1, 1], 0.3, 0.4), name=('w_class')) # unconstrained w\n",
    "    w = tf.Variable(tf.constant( [[0.3]] ), name=('w_class')) # fixed w\n",
    "    w = tf.concat(0, [-w, w])\n",
    "    \n",
    "    b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Define unrolling for training\n",
    "    train= ModelSetup( pos_net, neg_net, w, b, batch_size, num_unrollings, \n",
    "                       drop_prob_list, \n",
    "                       name='train_')\n",
    "       \n",
    "    # 3. Define unrolling for validation\n",
    "    if allow_valid:\n",
    "        valid= ModelSetup( pos_net, neg_net, w, b, batch_size_val, num_unrollings_val, \n",
    "                           drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                           name='valid_')\n",
    "        \n",
    "    # 4. Define unrolling for testing\n",
    "    if allow_test:\n",
    "        test= ModelSetup( pos_net, neg_net, w, b, batch_size_test, num_unrollings_test, \n",
    "                          drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                          name='test_')\n",
    "    \n",
    "    # 5. Define unrolling for testing generator\n",
    "    pos_gen_test, _= pos_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    neg_gen_test, _= neg_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    \n",
    "          \n",
    "    \n",
    "    # 6. Define optimizer    \n",
    "    # 1. specify the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) # ADAM 0.001\n",
    "    \n",
    "    # 2. get the gradients and variables\n",
    "    # grads_and_vars is a list of tuples (gradient, variable). \n",
    "    grads_and_vars = optimizer.compute_gradients(train.loss) \n",
    "    gradients, v = zip(*grads_and_vars)\n",
    "    \n",
    "    # 3. process the gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, grad_clip_thresh)  #1.25 #0.025 #0.001(last used)\n",
    "    # 4. apply the gradients to the optimization procedure\n",
    "    optimizer = optimizer.apply_gradients( zip(gradients, v) ) # ADAM\n",
    "    \n",
    "    # for prediction\n",
    "    train_pred = tf.nn.sigmoid(train.logits)\n",
    "    \n",
    "    # Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Energy predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_pos= False\n",
    "load_pos_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_pos.ckpt\"\n",
    "    \n",
    "reload_neg= False\n",
    "load_neg_file= working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_neg.ckpt\"\n",
    "\n",
    "reload_all= True\n",
    "load_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run-1) +\"r_All\"+comment+\".ckpt\";\n",
    "\n",
    "save_all= True\n",
    "save_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run) +\"r_All\"+comment+\".ckpt\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_run==1:\n",
    "    reload_all= False;\n",
    "else:\n",
    "    reload_pos= False;\n",
    "    reload_neg= False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Initialized\n",
      "step | Train_err | Valid_err | loss\n",
      "0 | 0.562500 | 0.513125 | 83.626427 \n",
      "50 | 0.339062 | 0.290313 | 69.138930 \n",
      "100 | 0.285313 | 0.309063 | 63.540941 \n",
      "150 | 0.375625 | 0.440937 | 62.096535 \n",
      "200 | 0.447813 | 0.445937 | 60.430718 \n",
      "250 | 0.415312 | 0.365625 | 59.081441 \n",
      "300 | 0.354687 | 0.329375 | 58.190073 \n",
      "350 | 0.318125 | 0.299375 | 57.072392 \n",
      "400 | 0.274375 | 0.283438 | 56.743268 | saved\n",
      "450 | 0.259062 | 0.270938 | 56.149739 | saved\n",
      "500 | 0.255312 | 0.247812 | 55.860060 | saved\n",
      "550 | 0.252812 | 0.253750 | 55.403851 \n",
      "600 | 0.252188 | 0.241563 | 55.331341 | saved\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "650 | 0.247188 | 0.252188 | 55.241735 \n",
      "700 | 0.257188 | 0.227500 | 54.826343 | saved\n",
      "750 | 0.207187 | 0.222812 | 54.636513 | saved\n",
      "800 | 0.232813 | 0.229063 | 54.447258 \n",
      "850 | 0.239063 | 0.222188 | 54.590777 | saved\n",
      "900 | 0.208437 | 0.213750 | 54.290748 | saved\n",
      "950 | 0.207187 | 0.214688 | 54.100558 \n",
      "1000 | 0.220000 | 0.222812 | 54.272898 \n",
      "1050 | 0.232813 | 0.211562 | 53.833904 | saved\n",
      "1100 | 0.224062 | 0.219688 | 54.089373 \n",
      "1150 | 0.211562 | 0.219375 | 53.698451 \n",
      "1200 | 0.208437 | 0.206250 | 53.668670 | saved\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1250 | 0.190938 | 0.208125 | 53.550194 \n",
      "1300 | 0.197813 | 0.204687 | 53.645682 | saved\n",
      "1350 | 0.180312 | 0.207187 | 53.448394 \n",
      "1400 | 0.185938 | 0.216562 | 53.041243 \n",
      "1450 | 0.185000 | 0.201563 | 53.133550 | saved\n",
      "1500 | 0.183125 | 0.219062 | 53.149167 \n",
      "1550 | 0.168125 | 0.216875 | 52.691361 \n",
      "1600 | 0.172813 | 0.196875 | 52.899265 | saved\n",
      "1650 | 0.160000 | 0.217500 | 52.609964 \n",
      "1700 | 0.167187 | 0.200625 | 52.632387 \n",
      "1750 | 0.167187 | 0.210625 | 52.319923 \n",
      "1800 | 0.166875 | 0.215312 | 52.497751 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1850 | 0.174375 | 0.203125 | 52.516176 \n",
      "1900 | 0.162812 | 0.210625 | 52.336065 \n",
      "1950 | 0.142500 | 0.204063 | 52.191062 \n",
      "2000 | 0.162188 | 0.190312 | 52.134868 | saved\n",
      "2050 | 0.166250 | 0.212812 | 52.368551 \n",
      "2100 | 0.143437 | 0.199687 | 52.272586 \n",
      "2150 | 0.144687 | 0.190000 | 52.051986 | saved\n",
      "2200 | 0.152500 | 0.205313 | 52.396024 \n",
      "2250 | 0.160312 | 0.192812 | 51.988207 \n",
      "2300 | 0.148750 | 0.208125 | 52.254490 \n",
      "2350 | 0.152500 | 0.201875 | 51.913100 \n",
      "2400 | 0.141250 | 0.194062 | 52.057032 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "2450 | 0.138750 | 0.209687 | 51.998082 \n",
      "2500 | 0.144375 | 0.192500 | 52.095381 \n",
      "2550 | 0.131250 | 0.194062 | 51.993031 \n",
      "2600 | 0.134063 | 0.207500 | 51.636560 \n",
      "2650 | 0.135625 | 0.187188 | 51.710087 | saved\n",
      "2700 | 0.130625 | 0.216250 | 51.777954 \n",
      "2750 | 0.118750 | 0.206563 | 51.486812 \n",
      "2800 | 0.119375 | 0.195625 | 51.493029 \n",
      "2850 | 0.103750 | 0.204687 | 51.417305 \n",
      "2900 | 0.117813 | 0.196250 | 51.465870 \n",
      "2950 | 0.112812 | 0.198437 | 51.173180 \n",
      "3000 | 0.115312 | 0.207187 | 51.294285 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "3050 | 0.128438 | 0.193750 | 51.373868 \n",
      "3100 | 0.125625 | 0.210625 | 51.323959 \n",
      "3150 | 0.095000 | 0.203125 | 51.129037 \n",
      "3200 | 0.107188 | 0.188125 | 51.038597 \n",
      "3250 | 0.116875 | 0.205938 | 51.302109 \n",
      "3300 | 0.110937 | 0.196875 | 51.326856 \n",
      "3350 | 0.098125 | 0.201250 | 50.982520 \n",
      "3400 | 0.101875 | 0.202500 | 51.421332 \n",
      "3450 | 0.118125 | 0.191250 | 51.016757 \n",
      "3500 | 0.109063 | 0.204063 | 51.274368 \n",
      "3550 | 0.103750 | 0.192500 | 50.995601 \n",
      "3600 | 0.099375 | 0.187812 | 51.168058 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "3650 | 0.083437 | 0.210312 | 51.044600 \n",
      "3700 | 0.101562 | 0.191250 | 51.262733 \n",
      "3750 | 0.078750 | 0.197187 | 51.165323 \n",
      "3800 | 0.095625 | 0.208750 | 50.716736 \n",
      "3850 | 0.094687 | 0.190625 | 50.952276 \n",
      "3900 | 0.090625 | 0.206875 | 50.953931 \n",
      "3950 | 0.081250 | 0.202500 | 50.776707 \n",
      "4000 | 0.080625 | 0.186875 | 50.619494 | saved\n",
      "4050 | 0.071875 | 0.201563 | 50.653515 \n",
      "4100 | 0.081562 | 0.200625 | 50.702192 \n",
      "4150 | 0.078750 | 0.194688 | 50.529223 \n",
      "4200 | 0.077813 | 0.204687 | 50.493416 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "4250 | 0.083437 | 0.194375 | 50.640624 \n",
      "4300 | 0.082812 | 0.201250 | 50.621350 \n",
      "4350 | 0.070312 | 0.195000 | 50.468695 \n",
      "4400 | 0.068125 | 0.190938 | 50.283814 \n",
      "4450 | 0.072188 | 0.197500 | 50.622915 \n",
      "4500 | 0.073750 | 0.209063 | 50.715835 \n",
      "4550 | 0.064062 | 0.200000 | 50.270900 \n",
      "4600 | 0.069062 | 0.206250 | 50.718980 \n",
      "4650 | 0.078750 | 0.194062 | 50.390735 \n",
      "4700 | 0.072813 | 0.205937 | 50.576415 \n",
      "4750 | 0.068750 | 0.204375 | 50.388965 \n",
      "4800 | 0.066875 | 0.190000 | 50.553083 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "4850 | 0.055625 | 0.202500 | 50.426000 \n",
      "4900 | 0.067812 | 0.200937 | 50.650660 \n",
      "4950 | 0.054688 | 0.195312 | 50.628902 \n",
      "5000 | 0.064375 | 0.206250 | 50.088806 \n",
      "5050 | 0.062187 | 0.193125 | 50.302925 \n",
      "5100 | 0.060312 | 0.204687 | 50.392420 \n",
      "5150 | 0.053437 | 0.206250 | 50.288316 \n",
      "5200 | 0.056250 | 0.184687 | 50.029962 | saved\n",
      "5250 | 0.046875 | 0.205000 | 50.107931 \n",
      "5300 | 0.049375 | 0.208750 | 50.166196 \n",
      "5350 | 0.044687 | 0.188437 | 49.994588 \n",
      "5400 | 0.054375 | 0.198437 | 49.957081 \n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "5450 | 0.050313 | 0.205937 | 50.129648 \n",
      "5500 | 0.050625 | 0.200313 | 50.142808 \n",
      "5550 | 0.040000 | 0.198750 | 50.007837 \n",
      "5600 | 0.045000 | 0.193750 | 49.707788 \n",
      "5650 | 0.048438 | 0.207500 | 50.164983 \n",
      "5700 | 0.047813 | 0.213125 | 50.233506 \n",
      "5750 | 0.040938 | 0.195000 | 49.752760 \n",
      "5800 | 0.047188 | 0.198750 | 50.163104 \n",
      "5850 | 0.050000 | 0.200625 | 49.925877 \n",
      "5900 | 0.047813 | 0.199687 | 50.191061 \n",
      "5950 | 0.046562 | 0.200313 | 49.952723 \n",
      "Learning finished, weights saved in file: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb/weights/Weights_LSTM_L2_25u_1r_All_noDropout_LcLpLcp_64unrol_standarloss.ckpt\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6000 \n",
    "summary_frequency = 50\n",
    "n_valid_tests = 50\n",
    "n_characters_step= num_unrollings*batch_size\n",
    "\n",
    "train_error_l= list()\n",
    "valid_error_l= list()\n",
    "\n",
    "aux_print=0\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # -------------------------------- Load weigths from file, or initialize variables ------------------------------------\n",
    "    if reload_all:\n",
    "        saver.restore(session, load_all_file)\n",
    "        #session.run( global_step.assign(0) ) # SGD\n",
    "        \n",
    "    elif (reload_pos and reload_neg):\n",
    "        tf.initialize_all_variables().run()\n",
    "        pos_net.saver.restore(session, load_pos_file)\n",
    "        neg_net.saver.restore(session, load_neg_file)\n",
    "        print('Weights for positive and negative networks loaded')\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Weights Initialized')\n",
    "    \n",
    "    # ------------------------------------------- Training loop --------------------------------------------------\n",
    "    print('step | Train_err | Valid_err | loss')\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    mean_error = 0.0\n",
    "    for step in range(num_steps):\n",
    "        # 1. Get next batch\n",
    "        batch_X, batch_y = dataset.train.next_batch()\n",
    "        \n",
    "        # 2. Setup feed dictionary for network 1 and network 2\n",
    "        feed_dict = dict()\n",
    "        # For dropout\n",
    "        feed_dict[drop_prob] = dropout_cons\n",
    "        # hyperparameters\n",
    "        feed_dict[train.alpha_r] = alpha\n",
    "        feed_dict[train.beta_r] = beta\n",
    "            \n",
    "        # Inroduce labels\n",
    "        feed_dict[train.labels] = batch_y\n",
    "        # Introduce inputs\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train.X[i]] = batch_X[i]\n",
    "              \n",
    "        # 3 Run optimizer\n",
    "        #_, l, lr = session.run( [optimizer, loss_train, learning_rate], feed_dict=feed_dict) # SGD\n",
    "        _, l, train_error = session.run( [optimizer, train.loss, train.error], feed_dict=feed_dict) # ADAM\n",
    "        \n",
    "        mean_loss += l\n",
    "        mean_error += train_error #classification_error(pred_train_aux, batch_y) \n",
    "        \n",
    "        # --------------------------------------------- logging ------------------------------------------------\n",
    "        if dataset.train.batch_generators[0]._text_size<aux_print :\n",
    "            print('+'*80)\n",
    "            print('')\n",
    "            aux_print = 0\n",
    "        else:\n",
    "            aux_print += n_characters_step\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                mean_error = mean_error / summary_frequency\n",
    "                        \n",
    "            # ---------- Print loss in validation dataset ---------\n",
    "            if allow_valid:\n",
    "                ''' classification error on validation dataset '''\n",
    "                mean_error_val = 0.0\n",
    "                for step_valid in range(n_valid_tests):\n",
    "                    # 1. Get next batch\n",
    "                    batch_X_val, batch_y_val = dataset.valid.next_batch()        \n",
    "                    feed_dict_val = dict()\n",
    "                    # For dropout\n",
    "                    feed_dict_val[drop_prob] = 1.0\n",
    "                    # Inroduce labels\n",
    "                    feed_dict_val[valid.labels] = batch_y_val\n",
    "                    # Introduce inputs\n",
    "                    for i in range(num_unrollings_val+1):\n",
    "                        feed_dict_val[valid.X[i]] = batch_X_val[i]\n",
    "                    # 2. Get classification error\n",
    "                    [valid_error] = session.run( [valid.error], feed_dict=feed_dict_val)          \n",
    "                    mean_error_val = mean_error_val*step_valid\n",
    "                    mean_error_val += valid_error\n",
    "                    mean_error_val = mean_error_val/(step_valid+1)\n",
    "                                    \n",
    "                ''' print information '''\n",
    "                train_error_l.append(mean_error)\n",
    "                valid_error_l.append(mean_error_val)\n",
    "                \n",
    "                if save_all and valid_error_l[-1] == min(valid_error_l) and step>200:\n",
    "                    save_path = saver.save(session, save_all_file)\n",
    "                    print('%d | %f | %f | %f | saved' % (step, \n",
    "                                                         mean_error, \n",
    "                                                         mean_error_val, \n",
    "                                                         mean_loss\n",
    "                                                        ))    \n",
    "                \n",
    "                else:\n",
    "                    print('%d | %f | %f | %f ' % (step, \n",
    "                                                  mean_error, \n",
    "                                                  mean_error_val, \n",
    "                                                  mean_loss\n",
    "                                                 ))    \n",
    "                    \n",
    "                #if(mean_error_val < 0.19):\n",
    "                #    break\n",
    "            else:\n",
    "                train_error_l.append(mean_error)\n",
    "                print('%d | %f | %f' % (step, mean_error, mean_loss))\n",
    "                                       \n",
    "            mean_loss = 0\n",
    "            mean_error = 0\n",
    "            \n",
    "    if save_all and allow_valid:\n",
    "        print(\"Learning finished, weights saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fadb401b518>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fadb401b400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "train_error_a = np.array(train_error_l)\n",
    "valid_error_a = np.array(valid_error_l)\n",
    "\n",
    "plt.plot(np.linspace(0, train_error_a.shape[0], train_error_a.shape[0]) , train_error_a )\n",
    "plt.plot(np.linspace(0, valid_error_a.shape[0], train_error_a.shape[0]) , valid_error_a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification error on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n",
      "Classification error on test dataset: 0.225703125\n"
     ]
    }
   ],
   "source": [
    "n_tests = 600\n",
    "if allow_test:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # Load weigths from file, or initialize variables\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver.restore(session, save_all_file)\n",
    "        print('Weights loaded')\n",
    "        \n",
    "        # calculate errors using mini-batches\n",
    "        mean_error_test = 0\n",
    "        for step_test in range(n_tests):\n",
    "            # 1. Get next batch\n",
    "            batch_X_test, batch_y_test = dataset.test.next_batch()        \n",
    "            feed_dict_test = dict()\n",
    "            # For dropout\n",
    "            feed_dict_test[drop_prob] = 1.0\n",
    "            # Inroduce labels\n",
    "            feed_dict_test[test.labels] = batch_y_test\n",
    "            # Introduce inputs\n",
    "            for i in range(num_unrollings_test+1):\n",
    "                feed_dict_test[test.X[i]] = batch_X_test[i]\n",
    "            # 2. Get prediction\n",
    "            [test_error] = session.run( [test.error], feed_dict=feed_dict_test)          \n",
    "            mean_error_test = mean_error_test*step_test\n",
    "            mean_error_test += test_error\n",
    "            mean_error_test = mean_error_test/(step_test+1)\n",
    "            \n",
    "        print(\"Classification error on test dataset:\", mean_error_test)\n",
    "        \n",
    "    ''' perplexity '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some samples from positive and negative networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights loaded\n",
      "b= [ 0.]\n",
      "w= [[-15.73105431]\n",
      " [ 15.73105431]]\n",
      "a------------------------------pos------------------------------\n",
      "act over my cast i are almost the onli film well dealt within the most popular most and the joke were wrong the mean of that lead to some great music \n",
      "rider at the show expos is the awesom are genuin as a far slow pace human when hi thi and when we realiz that there is the end of some surpris \n",
      "candi return problem situat figur it mock elvira not enough in the univers hand of despair so much from the way thi stori line lend a lot of love but with \n",
      "chop in for a larg minut i wa go to speak hold up high school glad they are everi bit properli be differ much about the german seri in i went \n",
      "flick and ha fallen with hi love for a young woman who decid he repeat some key comedi includ sam who style is on the voic in some of the film \n",
      "a--------------------------------------------------------------------------------\n",
      "a------------------------------neg------------------------------\n",
      "homicid trail is trap wise nake who with canada and then everyon count fish with the tragic figur one like o look far so the that rise me your gonna run \n",
      "star of the wood with the definit of hi skin high improv on i love the filmmak wa no expect that gotta do anyth els and brando s dream off who \n",
      "timothi give a scari movi as much as a fairli bad film and is but total shallow becaus the charact made by so quickli all out of drive your sudden know \n",
      "spiritu well as though if that men were alreadi told the director would ruin thi effort on becaus it not worth on it worth watch it s dark thi clich s \n",
      "empir wa earth and i m not sure one movi felt like thi way that just is one of the chines clich and the script is from sheer mood the direct \n",
      "a--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Load weigths from file, or initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(session, save_all_file)\n",
    "    print('Weights loaded')\n",
    "        \n",
    "    # print values for b and w\n",
    "    b_t= b.eval()\n",
    "    w_t= w.eval()\n",
    "    #b_t= session.run(b)\n",
    "    print('b=', b_t)\n",
    "    print('w=', w_t)\n",
    "    \n",
    "    \n",
    "    # For positive network\n",
    "    print('a' + '-'*30 + 'pos' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        pos_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(pos_gen_test.y).eval({pos_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)\n",
    "    \n",
    "    # For negative network\n",
    "    print('a' + '-'*30 + 'neg' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        neg_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(neg_gen_test.y).eval({neg_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
