{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous generation-classification using LSTM \n",
    "Implementation of the paper: Simultaneous Generation-classification using lstm \n",
    "\n",
    "Authors: Daniel L. Marino, Kasun Amarasinghe, Milos Manic\n",
    "\n",
    "This script implements the training using Generation and Classification as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#************************************************************************\n",
    "#      __   __  _    _  _____   _____\n",
    "#     /  | /  || |  | ||     \\ /  ___|\n",
    "#    /   |/   || |__| ||    _||  |  _\n",
    "#   / /|   /| ||  __  || |\\ \\ |  |_| |\n",
    "#  /_/ |_ / |_||_|  |_||_| \\_\\|______|\n",
    "#    \n",
    "#\n",
    "#   Copyright (2016) Modern Heuristics Research Group (MHRG)\n",
    "#   Virginia Commonwealth University (VCU), Richmond, VA\n",
    "#   http://www.people.vcu.edu/~mmanic/\n",
    "#   \n",
    "#   This program is free software: you can redistribute it and/or modify\n",
    "#   it under the terms of the GNU General Public License as published by\n",
    "#   the Free Software Foundation, either version 3 of the License, or\n",
    "#   (at your option) any later version.\n",
    "#\n",
    "#   This program is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#   GNU General Public License for more details.\n",
    "#  \n",
    "#   Any opinions, findings, and conclusions or recommendations expressed \n",
    "#   in this material are those of the author's(s') and do not necessarily \n",
    "#   reflect the views of any other entity.\n",
    "#  \n",
    "#   ***********************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "import time\n",
    "\n",
    "from twodlearn.tf_lib.Feedforward import LinearLayer\n",
    "from twodlearn.tf_lib.Recurrent import *\n",
    "\n",
    "import sys\n",
    "working_dir= os.getcwd()\n",
    "\n",
    "print('Working directory:', working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allow_gpu= False\n",
    "allow_valid= True\n",
    "allow_test= True\n",
    "Valid_Percentage= 0.1 # i.e. 10% of the data will be used for validation\n",
    "\n",
    "pos_net_name= '_pos'\n",
    "neg_net_name= '_neg'\n",
    "\n",
    "activation_function='tanh'\n",
    "num_nodes = [100, 100] #num_nodes: Nodes for the LSTM cell\n",
    "alpha = 10.0 #0.1 #0.1\n",
    "beta = 0.1 #0.01 #10000.01\n",
    "lambda_w = 0.00001\n",
    "\n",
    "dropout_cons = 0.8\n",
    "\n",
    "Allow_Bias= False \n",
    "\n",
    "learning_rate= 0.005      # 0.001\n",
    "grad_clip_thresh= 1.1       # 0.00001\n",
    "\n",
    "current_run= 1\n",
    "batch_size= 64 #64\n",
    "num_unrollings= 64 #100\n",
    "\n",
    "batch_size_val= 64 #len(valid_text_pos)/num_unrollings_val #500\n",
    "num_unrollings_val= num_unrollings #100\n",
    "\n",
    "batch_size_test= 64 \n",
    "num_unrollings_test= num_unrollings #100\n",
    "\n",
    "comment='_noDropout_LcLpLcp_64unrol_standarloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: [100, 100]\n",
      "alpha: 10.0 , beta: 0.1 , lambda_w: 1e-05\n",
      "num_unrollings: 64 , batch_size: 64 , batch_size_val: 64\n",
      "learning_rate: 0.005 grad_clip_thresh: 1.1\n"
     ]
    }
   ],
   "source": [
    "model_version = 'L'+str(len(num_nodes))\n",
    "print(\"num_nodes:\",num_nodes)\n",
    "print(\"alpha:\",alpha, \", beta:\",beta,\", lambda_w:\", lambda_w)\n",
    "print(\"num_unrollings:\",num_unrollings, \", batch_size:\",batch_size,\", batch_size_val:\", batch_size_val)\n",
    "print(\"learning_rate:\", learning_rate, 'grad_clip_thresh:', grad_clip_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "(64, 1)\n",
      "(64, 4001)\n",
      "lame werewolf suit with the function i mean it just plain hard to sit through not to mention the act is terribl and the soundtrack is dub badli pleas i know the cover is interest what look like a hand reach for a woman s bare ass but don t wast your time or money as you won t get either back revolut alway present \n",
      "[ 0.]\n",
      "and skip over when she is rape by four we see onli the threat and then betti recov and surviv pick herself up in desert and put on the brave face of someon who refus to lie down and die although no nuditi is involv it s betti s special interest photo that eventu troubl when we go back to the court the opinion of \n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "vc= pickle.load( open( \"imdb_vc.pkl\", \"rb\" ) )\n",
    "num_inputs=  vc.vocabulary_size\n",
    "num_outputs= vc.vocabulary_size\n",
    "\n",
    "dataset= pickle.load( open( \"imdb_dataset.pkl\", \"rb\" ) )\n",
    "\n",
    "# set batch_size and number of unrolligns\n",
    "dataset.train.set_batch_and_unrollings(batch_size, num_unrollings)\n",
    "dataset.valid.set_batch_and_unrollings(batch_size_val, num_unrollings_val)\n",
    "dataset.test.set_batch_and_unrollings(batch_size_test, num_unrollings_test)\n",
    "\n",
    "# print a sample of the dataset\n",
    "train_x, train_y= dataset.train.next_batch()\n",
    "print(len(train_x))\n",
    "print(train_y.shape)\n",
    "print(train_x[0].shape)\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][0,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[0])\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][50,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[50])\n",
    "\n",
    "valid_x, valid_y= dataset.valid.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs: 4001\n",
      "pos train length: 2467781\n",
      "neg train length: 2467781\n",
      "pos valid length: 274197\n",
      "neg valid length: 274197\n",
      "pos test length: 2686211\n",
      "neg test length: 2686211\n"
     ]
    }
   ],
   "source": [
    "print('# inputs:',num_inputs)\n",
    "\n",
    "print('pos train length:',dataset.train.batch_generators[0]._text_size )\n",
    "print('neg train length:',dataset.train.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos valid length:',dataset.valid.batch_generators[0]._text_size )\n",
    "print('neg valid length:',dataset.valid.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos test length:',dataset.test.batch_generators[0]._text_size )\n",
    "print('neg test length:',dataset.test.batch_generators[1]._text_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_error(predictions, labels):\n",
    "    \"\"\" number of samples wrongly classified  \"\"\"\n",
    "    return np.sum( np.not_equal( np.greater(predictions, 0.5), \n",
    "                                 np.greater(labels, 0.5)) )/labels.shape[0]\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10 # this is to prevent that log() returns minus infinity\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vc.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vc.vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myLstmNet(LstmNet):\n",
    "    def get_extra_inputs(self, i, h_list, state_list):\n",
    "        #print('OK:', len(h_list))\n",
    "        return i\n",
    "    \n",
    "    def evaluate_final_output(self, outputs_list, inputs_list, h_list ):\n",
    "        ''' Calculates the final output of the neural network, usually it is just a linear transformation\n",
    "            - outputs_list: list with the outputs from the last lstm cell\n",
    "            - inputs_list: list of inputs to the network\n",
    "            - h_list: list with all hidden outputs from all the cells\n",
    "        '''\n",
    "        all_hidden = list()\n",
    "        \n",
    "        for t in h_list: # go trough each time step\n",
    "            all_hidden.append( tf.concat(1,t) )\n",
    "        return self.out_layer.evaluate(tf.concat(0, all_hidden))  \n",
    "            \n",
    "    \n",
    "if len(num_nodes)>1:\n",
    "    n_extra= [num_inputs for i in range(len(num_nodes)+1)]\n",
    "    n_extra[0]= 0\n",
    "    n_extra[-1]= sum(num_nodes) - num_nodes[-1]\n",
    "else:\n",
    "    n_extra= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    \n",
    "    def __init__( self, pos_net, neg_net, w, b, batch_size, num_unrollings, drop_prob_list, name=''):\n",
    "    \n",
    "        # 1. Create placeholders for inputs \n",
    "        self.X = list()\n",
    "        for iaux in range(num_unrollings + 1):\n",
    "            self.X.append(tf.placeholder(tf.float32, shape=[batch_size, vc.vocabulary_size], \n",
    "                                             name= name+'X_i'+str(iaux)+'_All'))\n",
    "        aux_inputs = self.X[:num_unrollings]\n",
    "        aux_labels = self.X[1:]  # inputs shifted by one time step.\n",
    "\n",
    "        # Create a list for store the placeholders for the labels\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "       \n",
    "\n",
    "        # -------------------- unrolling of the network --------------------------- # \n",
    "        self.pos_unroll, _= pos_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   )\n",
    "\n",
    "        self.neg_unroll, _= neg_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        # error_per_sample is a vector, its shape is changed to have each unrolling in separate columns\n",
    "        output_pos= tf.reshape(self.pos_unroll.error_per_sample,[num_unrollings,batch_size])  \n",
    "        output_neg= tf.reshape(self.neg_unroll.error_per_sample,[num_unrollings,batch_size]) \n",
    "\n",
    "        output_pos_mean= tf.reduce_mean(output_pos, reduction_indices= 0) \n",
    "        output_neg_mean= tf.reduce_mean(output_neg, reduction_indices= 0) \n",
    "\n",
    "        output_mean= tf.transpose(tf.pack( [ output_pos_mean, output_neg_mean ]))\n",
    "\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "\n",
    "\n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        \n",
    "        # prediction error\n",
    "        #Lp = tf.reduce_mean( tf.mul(self.labels, output_pos_mean) + tf.mul(self.labels-1, output_neg_mean))\n",
    "        Lp = tf.reduce_mean( tf.mul(tf.squeeze(self.labels), output_pos_mean) + \n",
    "                             tf.mul(tf.squeeze(1-self.labels), output_neg_mean)\n",
    "                           )\n",
    "        # c-p penalty\n",
    "        Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.exp(-output_pos_mean)) + \n",
    "                              tf.mul(tf.squeeze(self.labels), tf.exp(-output_neg_mean))\n",
    "                            )\n",
    "        \n",
    "        #Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.reduce_mean(tf.exp(-output_pos), reduction_indices= 0) ) + \n",
    "        #                      tf.mul(tf.squeeze(self.labels), tf.reduce_mean(tf.exp(-output_neg), reduction_indices= 0) )\n",
    "        #                    )\n",
    "        \n",
    "        # regularization\n",
    "        l2_c= tf.nn.l2_loss(w)\n",
    "        \n",
    "        # loss\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp + beta*(1.0/Lcp)\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp - beta*(Lcp)\n",
    "        self.alpha_r = tf.placeholder(tf.float32)\n",
    "        self.beta_r = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.loss = tf.reduce_mean( self.error_per_sample ) + self.alpha_r*Lp + self.beta_r*Lcp + lambda_w*l2_c\n",
    "        \n",
    "        # classification error\n",
    "        self.error = tf.reduce_mean( tf.to_float(tf.not_equal( tf.greater(tf.nn.sigmoid(self.logits), 0.5), \n",
    "                                                               tf.greater(self.labels, 0.5))) )\n",
    "        \n",
    "        # perplexity measure\n",
    "        next_words_prob_pos = -tf.log( tf.nn.softmax( self.pos_unroll.y ) + 1e-10)\n",
    "        next_words_prob_neg = -tf.log( tf.nn.softmax( self.neg_unroll.y ) + 1e-10)\n",
    "            \n",
    "        next_words=  tf.concat(0,aux_labels) \n",
    "        \n",
    "        logprob_pos= tf.reshape( tf.reduce_sum( next_words_prob_pos * next_words, 1), [batch_size, num_unrollings] )\n",
    "        logprob_neg= tf.reshape( tf.reduce_sum( next_words_prob_neg * next_words, 1), [batch_size, num_unrollings] )  \n",
    "               \n",
    "        #self.perplexity_pos( -tf.log( prob_pos + 1e-10 ) * self.labels )\n",
    "        #self.perplexity_neg( -tf.log( prob_neg + 1e-10 ) * (1-self.labels) )\n",
    "        self.perplexity_pos = tf.reduce_sum( logprob_pos * self.labels )/(batch_size/2.0) \n",
    "        self.perplexity_neg = tf.reduce_sum( logprob_neg * (1-self.labels) )/(batch_size/2.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # For dropout\n",
    "    drop_prob = tf.placeholder(tf.float32)\n",
    "    drop_prob_list = [ drop_prob for i in range(len(num_nodes)+1)]\n",
    "    drop_prob_list[0]= None\n",
    "     \n",
    "    # 1. Define positive and negative neural networks\n",
    "    pos_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= pos_net_name)\n",
    "        \n",
    "    neg_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= neg_net_name)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    #w = tf.Variable(tf.truncated_normal([2, 1], -0.1, 0.1), name=('w_class')) # unconstrained w\n",
    "    #w = tf.Variable(tf.constant( [[-2.0], [2.0]]), name=('w_class'), trainable=False) # fixed w\n",
    "    \n",
    "    #w = tf.Variable(tf.truncated_normal([1, 1], 0.3, 0.4), name=('w_class')) # unconstrained w\n",
    "    w = tf.Variable(tf.constant( [[0.3]] ), name=('w_class')) # fixed w\n",
    "    w = tf.concat(0, [-w, w])\n",
    "    \n",
    "    b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Define unrolling for training\n",
    "    train= ModelSetup( pos_net, neg_net, w, b, batch_size, num_unrollings, \n",
    "                       drop_prob_list, \n",
    "                       name='train_')\n",
    "       \n",
    "    # 3. Define unrolling for validation\n",
    "    if allow_valid:\n",
    "        valid= ModelSetup( pos_net, neg_net, w, b, batch_size_val, num_unrollings_val, \n",
    "                           drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                           name='valid_')\n",
    "        \n",
    "    # 4. Define unrolling for testing\n",
    "    if allow_test:\n",
    "        test= ModelSetup( pos_net, neg_net, w, b, batch_size_test, num_unrollings_test, \n",
    "                          drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                          name='test_')\n",
    "    \n",
    "    # 5. Define unrolling for testing generator\n",
    "    pos_gen_test, _= pos_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    neg_gen_test, _= neg_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    \n",
    "          \n",
    "    \n",
    "    # 6. Define optimizer    \n",
    "    # 1. specify the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) # ADAM 0.001\n",
    "    \n",
    "    # 2. get the gradients and variables\n",
    "    # grads_and_vars is a list of tuples (gradient, variable). \n",
    "    grads_and_vars = optimizer.compute_gradients(train.loss) \n",
    "    gradients, v = zip(*grads_and_vars)\n",
    "    \n",
    "    # 3. process the gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, grad_clip_thresh)  #1.25 #0.025 #0.001(last used)\n",
    "    # 4. apply the gradients to the optimization procedure\n",
    "    optimizer = optimizer.apply_gradients( zip(gradients, v) ) # ADAM\n",
    "    \n",
    "    # for prediction\n",
    "    train_pred = tf.nn.sigmoid(train.logits)\n",
    "    \n",
    "    # Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Energy predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_pos= False\n",
    "load_pos_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_pos.ckpt\"\n",
    "    \n",
    "reload_neg= False\n",
    "load_neg_file= working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_neg.ckpt\"\n",
    "\n",
    "reload_all= True\n",
    "load_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run-1) +\"r_All\"+comment+\".ckpt\";\n",
    "\n",
    "save_all= False\n",
    "save_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run) +\"r_All\"+comment+\".ckpt\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_run==1:\n",
    "    reload_all= False;\n",
    "else:\n",
    "    reload_pos= False;\n",
    "    reload_neg= False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Initialized\n",
      "step | Train_err | Valid_err | loss\n",
      "0 | 0.437500 | 0.495937 | 83.809677 | 27.665865\n",
      "50 | 0.350938 | 0.355000 | 68.584866 | 117.532119\n",
      "100 | 0.233750 | 0.307812 | 62.821073 | 207.062450\n",
      "150 | 0.297812 | 0.355625 | 60.422969 | 296.371388\n",
      "200 | 0.232187 | 0.285000 | 57.596723 | 385.681579\n",
      "250 | 0.231250 | 0.256563 | 56.724815 | 474.956008\n",
      "300 | 0.209687 | 0.255937 | 55.683403 | 564.298408\n",
      "350 | 0.205937 | 0.271562 | 54.767180 | 653.472715\n",
      "400 | 0.217500 | 0.262813 | 54.455047 | 742.875602\n",
      "450 | 0.220000 | 0.262188 | 53.846041 | 833.204787\n",
      "500 | 0.194375 | 0.261562 | 53.410270 | 922.962594\n",
      "550 | 0.193125 | 0.248750 | 53.107418 | 1012.564749\n",
      "600 | 0.199375 | 0.243125 | 52.838197 | 1102.210543\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "650 | 0.186875 | 0.235313 | 52.355610 | 1192.027143\n",
      "700 | 0.187812 | 0.241875 | 52.810201 | 1280.932535\n",
      "750 | 0.193125 | 0.232813 | 52.185216 | 1370.462742\n",
      "800 | 0.180312 | 0.229687 | 51.955879 | 1459.645214\n",
      "850 | 0.167187 | 0.234687 | 51.794711 | 1549.677105\n",
      "900 | 0.187812 | 0.225625 | 51.949473 | 1639.463693\n",
      "950 | 0.184688 | 0.222500 | 51.450227 | 1728.547280\n",
      "1000 | 0.188125 | 0.227813 | 51.183392 | 1817.778793\n",
      "1050 | 0.186250 | 0.213750 | 51.317456 | 1906.747413\n",
      "1100 | 0.193438 | 0.227500 | 51.177369 | 1995.828118\n",
      "1150 | 0.182812 | 0.210938 | 50.767123 | 2085.785291\n",
      "1200 | 0.170937 | 0.233125 | 50.650711 | 2174.849745\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1250 | 0.180312 | 0.234687 | 50.760567 | 2263.872809\n",
      "1300 | 0.160000 | 0.219688 | 50.847615 | 2353.537253\n",
      "1350 | 0.152812 | 0.221875 | 50.496177 | 2442.835979\n",
      "1400 | 0.144375 | 0.236250 | 50.000257 | 2531.965045\n",
      "1450 | 0.157500 | 0.218125 | 50.628610 | 2621.183787\n",
      "1500 | 0.131562 | 0.219062 | 50.282757 | 2710.055253\n",
      "1550 | 0.125312 | 0.221875 | 49.776960 | 2799.177325\n",
      "1600 | 0.125312 | 0.221875 | 49.961956 | 2888.788356\n",
      "1650 | 0.129375 | 0.233437 | 49.603124 | 2977.782443\n",
      "1700 | 0.104063 | 0.223438 | 49.246712 | 3067.054078\n",
      "1750 | 0.100312 | 0.220000 | 49.275406 | 3156.473816\n",
      "1800 | 0.103438 | 0.231875 | 49.162724 | 3245.464372\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1850 | 0.091875 | 0.224375 | 48.776228 | 3334.698990\n",
      "1900 | 0.090313 | 0.231875 | 49.316114 | 3423.560237\n",
      "1950 | 0.084062 | 0.216875 | 49.014025 | 3512.977670\n",
      "2000 | 0.078125 | 0.213438 | 48.837514 | 3602.201031\n",
      "2050 | 0.065937 | 0.225938 | 48.635512 | 3692.449217\n",
      "2100 | 0.080312 | 0.214375 | 49.082573 | 3782.053632\n",
      "2150 | 0.070000 | 0.216875 | 48.634131 | 3871.173005\n",
      "2200 | 0.070000 | 0.223125 | 48.382429 | 3959.762996\n",
      "2250 | 0.061875 | 0.215312 | 48.537789 | 4048.546997\n",
      "2300 | 0.068437 | 0.234687 | 48.495315 | 4137.754658\n",
      "2350 | 0.058438 | 0.215312 | 48.248515 | 4226.384705\n",
      "2400 | 0.050625 | 0.219688 | 48.094000 | 4315.423834\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "2450 | 0.058750 | 0.241250 | 48.275159 | 4404.192839\n",
      "2500 | 0.051250 | 0.217500 | 48.373057 | 4493.262476\n",
      "2550 | 0.043750 | 0.213750 | 48.237153 | 4582.099784\n",
      "2600 | 0.047188 | 0.217188 | 47.943404 | 4671.030710\n",
      "2650 | 0.038750 | 0.215938 | 48.311518 | 4760.063884\n",
      "2700 | 0.038750 | 0.227187 | 48.129000 | 4848.982723\n",
      "2750 | 0.038125 | 0.219688 | 47.686607 | 4938.102643\n",
      "2800 | 0.035625 | 0.222500 | 48.080382 | 5027.048117\n",
      "2850 | 0.033125 | 0.235000 | 47.676698 | 5115.866572\n",
      "2900 | 0.031563 | 0.235625 | 47.269122 | 5204.754981\n",
      "2950 | 0.027813 | 0.219375 | 47.480896 | 5293.492859\n",
      "3000 | 0.026875 | 0.234063 | 47.290244 | 5382.398442\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "3050 | 0.025000 | 0.226250 | 47.026962 | 5470.940142\n",
      "3100 | 0.018750 | 0.231875 | 47.477401 | 5559.838728\n",
      "3150 | 0.015938 | 0.223125 | 47.379208 | 5649.073012\n",
      "3200 | 0.013437 | 0.227813 | 47.161671 | 5738.267277\n",
      "3250 | 0.012812 | 0.235313 | 46.944999 | 5827.272104\n",
      "3300 | 0.013125 | 0.210625 | 47.456016 | 5916.331150\n",
      "3350 | 0.016875 | 0.231250 | 47.071256 | 6005.430805\n",
      "3400 | 0.012188 | 0.228750 | 46.783753 | 6094.471436\n",
      "3450 | 0.011875 | 0.224062 | 46.934158 | 6183.249681\n",
      "3500 | 0.013125 | 0.240625 | 46.997066 | 6271.929569\n",
      "3550 | 0.009687 | 0.223438 | 46.713095 | 6360.846112\n",
      "3600 | 0.010937 | 0.222812 | 46.547201 | 6449.668357\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "3650 | 0.013125 | 0.235625 | 46.829763 | 6538.634790\n",
      "3700 | 0.007812 | 0.228125 | 46.924709 | 6627.803992\n",
      "3750 | 0.006250 | 0.219375 | 46.829286 | 6716.744896\n",
      "3800 | 0.007812 | 0.230000 | 46.609797 | 6805.378995\n",
      "3850 | 0.007812 | 0.214375 | 46.878988 | 6894.183574\n",
      "3900 | 0.004063 | 0.235625 | 46.779400 | 6983.660833\n",
      "3950 | 0.005000 | 0.234687 | 46.501947 | 7072.521415\n",
      "4000 | 0.006250 | 0.225312 | 46.686984 | 7161.470251\n",
      "4050 | 0.006562 | 0.243750 | 46.615659 | 7250.181156\n",
      "4100 | 0.004063 | 0.223750 | 46.048983 | 7339.764891\n",
      "4150 | 0.005000 | 0.235313 | 46.266961 | 7428.611035\n",
      "4200 | 0.005000 | 0.238437 | 46.186678 | 7517.422908\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "4250 | 0.003438 | 0.225000 | 45.960608 | 7606.400402\n",
      "4300 | 0.003438 | 0.243438 | 46.273394 | 7695.649331\n",
      "4350 | 0.005000 | 0.230937 | 46.299418 | 7784.603422\n",
      "4400 | 0.001875 | 0.230000 | 46.062193 | 7873.350497\n",
      "4450 | 0.002812 | 0.235625 | 45.830633 | 7962.190997\n",
      "4500 | 0.001875 | 0.232500 | 46.478613 | 8051.045363\n",
      "4550 | 0.002500 | 0.228437 | 45.984569 | 8139.930143\n",
      "4600 | 0.002188 | 0.232813 | 45.832257 | 8229.082105\n",
      "4650 | 0.002812 | 0.231250 | 45.891076 | 8317.990256\n",
      "4700 | 0.001563 | 0.249062 | 46.042470 | 8406.677012\n",
      "4750 | 0.001250 | 0.239063 | 45.699833 | 8495.712703\n",
      "4800 | 0.001250 | 0.228750 | 45.593053 | 8584.580794\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "4850 | 0.002812 | 0.249062 | 45.879166 | 8673.583371\n",
      "4900 | 0.001875 | 0.239063 | 46.006952 | 8762.405651\n",
      "4950 | 0.000937 | 0.222188 | 45.918332 | 8851.774015\n",
      "5000 | 0.000937 | 0.242500 | 45.716699 | 8940.613567\n",
      "5050 | 0.001563 | 0.236563 | 45.941788 | 9029.570504\n",
      "5100 | 0.001250 | 0.232187 | 45.851990 | 9118.514596\n",
      "5150 | 0.000625 | 0.242188 | 45.741930 | 9207.459222\n",
      "5200 | 0.001563 | 0.235313 | 45.720643 | 9296.291204\n",
      "5250 | 0.001563 | 0.249375 | 45.835188 | 9385.201327\n",
      "5300 | 0.000313 | 0.223438 | 45.255946 | 9473.879536\n",
      "5350 | 0.002500 | 0.230625 | 45.415597 | 9562.957329\n",
      "5400 | 0.000937 | 0.257188 | 45.403760 | 9651.685623\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "5450 | 0.001250 | 0.229375 | 45.211769 | 9740.419222\n",
      "5500 | 0.000313 | 0.247812 | 45.478605 | 9829.341092\n",
      "5550 | 0.000937 | 0.234687 | 45.456494 | 9918.214717\n",
      "5600 | 0.000625 | 0.242812 | 45.336512 | 10007.383646\n",
      "5650 | 0.000313 | 0.237813 | 45.103590 | 10096.581273\n",
      "5700 | 0.000937 | 0.233750 | 45.717952 | 10185.720218\n",
      "5750 | 0.000625 | 0.236250 | 45.269129 | 10274.662077\n",
      "5800 | 0.000313 | 0.246250 | 45.170464 | 10363.625822\n",
      "5850 | 0.000937 | 0.229063 | 45.152510 | 10452.819781\n",
      "5900 | 0.000313 | 0.238750 | 45.323076 | 10541.559515\n",
      "5950 | 0.000625 | 0.244062 | 45.020819 | 10630.485908\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6000 \n",
    "summary_frequency = 50\n",
    "n_valid_tests = 50\n",
    "n_characters_step= num_unrollings*batch_size\n",
    "\n",
    "train_error_l= list()\n",
    "valid_error_l= list()\n",
    "\n",
    "if allow_gpu:\n",
    "    config= None\n",
    "else:\n",
    "    config = tf.ConfigProto( device_count = {'GPU': 0} )\n",
    "\n",
    "aux_print=0\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    # -------------------------------- Load weigths from file, or initialize variables ------------------------------------\n",
    "    if reload_all:\n",
    "        saver.restore(session, load_all_file)\n",
    "        #session.run( global_step.assign(0) ) # SGD\n",
    "        \n",
    "    elif (reload_pos and reload_neg):\n",
    "        tf.initialize_all_variables().run()\n",
    "        pos_net.saver.restore(session, load_pos_file)\n",
    "        neg_net.saver.restore(session, load_neg_file)\n",
    "        print('Weights for positive and negative networks loaded')\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Weights Initialized')\n",
    "    \n",
    "    # ------------------------------------------- Training loop --------------------------------------------------\n",
    "    print('step | Train_err | Valid_err | loss')\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    mean_error = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # 1. Get next batch\n",
    "        batch_X, batch_y = dataset.train.next_batch()\n",
    "        \n",
    "        # 2. Setup feed dictionary for network 1 and network 2\n",
    "        feed_dict = dict()\n",
    "        # For dropout\n",
    "        feed_dict[drop_prob] = dropout_cons\n",
    "        # hyperparameters\n",
    "        feed_dict[train.alpha_r] = alpha\n",
    "        feed_dict[train.beta_r] = beta\n",
    "            \n",
    "        # Inroduce labels\n",
    "        feed_dict[train.labels] = batch_y\n",
    "        # Introduce inputs\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train.X[i]] = batch_X[i]\n",
    "              \n",
    "        # 3 Run optimizer\n",
    "        #_, l, lr = session.run( [optimizer, loss_train, learning_rate], feed_dict=feed_dict) # SGD\n",
    "        _, l, train_error = session.run( [optimizer, train.loss, train.error], feed_dict=feed_dict) # ADAM\n",
    "        \n",
    "        mean_loss += l\n",
    "        mean_error += train_error #classification_error(pred_train_aux, batch_y) \n",
    "        \n",
    "        # --------------------------------------------- logging ------------------------------------------------\n",
    "        if dataset.train.batch_generators[0]._text_size<aux_print :\n",
    "            print('+'*80)\n",
    "            print('')\n",
    "            aux_print = 0\n",
    "        else:\n",
    "            aux_print += n_characters_step\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                mean_error = mean_error / summary_frequency\n",
    "                        \n",
    "            # ---------- Print loss in validation dataset ---------\n",
    "            if allow_valid:\n",
    "                ''' classification error on validation dataset '''\n",
    "                mean_error_val = 0.0\n",
    "                for step_valid in range(n_valid_tests):\n",
    "                    # 1. Get next batch\n",
    "                    batch_X_val, batch_y_val = dataset.valid.next_batch()        \n",
    "                    feed_dict_val = dict()\n",
    "                    # For dropout\n",
    "                    feed_dict_val[drop_prob] = 1.0\n",
    "                    # Inroduce labels\n",
    "                    feed_dict_val[valid.labels] = batch_y_val\n",
    "                    # Introduce inputs\n",
    "                    for i in range(num_unrollings_val+1):\n",
    "                        feed_dict_val[valid.X[i]] = batch_X_val[i]\n",
    "                    # 2. Get classification error\n",
    "                    [valid_error] = session.run( [valid.error], feed_dict=feed_dict_val)          \n",
    "                    mean_error_val += valid_error\n",
    "                    \n",
    "                mean_error_val = mean_error_val/n_valid_tests\n",
    "                \n",
    "                ''' print information '''\n",
    "                train_error_l.append(mean_error)\n",
    "                valid_error_l.append(mean_error_val)\n",
    "                \n",
    "                if save_all and valid_error_l[-1] == min(valid_error_l) and step>200:\n",
    "                    save_path = saver.save(session, save_all_file)\n",
    "                    print('%d | %f | %f | %f | saved' % (step, \n",
    "                                                         mean_error, \n",
    "                                                         mean_error_val, \n",
    "                                                         mean_loss\n",
    "                                                        ))    \n",
    "                \n",
    "                else:\n",
    "                    cur_time = time.time() - start_time\n",
    "                    print('%d | %f | %f | %f | %f' % (step, \n",
    "                                                      mean_error, \n",
    "                                                      mean_error_val, \n",
    "                                                      mean_loss,\n",
    "                                                      cur_time\n",
    "                                                 ))   \n",
    "                    \n",
    "                #if(mean_error_val < 0.19):\n",
    "                #    break\n",
    "            else:\n",
    "                train_error_l.append(mean_error)\n",
    "                print('%d | %f | %f' % (step, mean_error, mean_loss))\n",
    "                                       \n",
    "            mean_loss = 0\n",
    "            mean_error = 0\n",
    "            \n",
    "    if save_all and allow_valid:\n",
    "        print(\"Learning finished, weights saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f8de79bc6a0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEACAYAAAC9Gb03AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nF0IIO/d/yMKDbnctJmAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8de79bc898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "train_error_a = np.array(train_error_l)\n",
    "valid_error_a = np.array(valid_error_l)\n",
    "\n",
    "plt.plot(np.linspace(0, train_error_a.shape[0], train_error_a.shape[0]) , train_error_a )\n",
    "plt.plot(np.linspace(0, valid_error_a.shape[0], train_error_a.shape[0]) , valid_error_a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification error on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb/weights",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-6ff9810ede13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# Load weigths from file, or initialize variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_all_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_all_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Weights loaded'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marinodl/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[1;34m(self, sess, save_path)\u001b[0m\n\u001b[0;32m   1100\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdoes\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \"\"\"\n\u001b[1;32m-> 1102\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_matching_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Restore called with invalid save path %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m     sess.run(self.saver_def.restore_op_name,\n",
      "\u001b[1;32m/home/marinodl/anaconda3/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36mget_matching_files\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    455\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mGetMatchingFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marinodl/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/marinodl/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/errors.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[1;34m()\u001b[0m\n\u001b[0;32m    448\u001b[0m           \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[0;32m    451\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_DeleteStatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb/weights"
     ]
    }
   ],
   "source": [
    "n_tests = 600\n",
    "if allow_test:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # Load weigths from file, or initialize variables\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver.restore(session, save_all_file)\n",
    "        print('Weights loaded')\n",
    "        \n",
    "        # calculate errors using mini-batches\n",
    "        mean_error_test = 0\n",
    "        mean_perp_pos = 0\n",
    "        mean_perp_neg = 0\n",
    "        for step_test in range(n_tests):\n",
    "            # 1. Get next batch\n",
    "            batch_X_test, batch_y_test = dataset.test.next_batch()        \n",
    "            feed_dict_test = dict()\n",
    "            # For dropout\n",
    "            feed_dict_test[drop_prob] = 1.0\n",
    "            # Inroduce labels\n",
    "            feed_dict_test[test.labels] = batch_y_test\n",
    "            # Introduce inputs\n",
    "            for i in range(num_unrollings_test+1):\n",
    "                feed_dict_test[test.X[i]] = batch_X_test[i]\n",
    "            # 2. Get prediction\n",
    "            [test_error, perp_pos, perp_neg] = session.run( [test.error, test.perplexity_pos, test.perplexity_neg], \n",
    "                                                            feed_dict=feed_dict_test)          \n",
    "            mean_error_test += test_error\n",
    "            mean_perp_pos += perp_pos\n",
    "            mean_perp_neg += perp_neg\n",
    "            \n",
    "        print(\"Classification error on test dataset:\", mean_error_test)\n",
    "        print(\"Perplexity on positive dataset:\", mean_perp_pos)\n",
    "        print(\"Perplexity on negative dataset:\", mean_perp_neg)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some samples from positive and negative networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Load weigths from file, or initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(session, save_all_file)\n",
    "    print('Weights loaded')\n",
    "        \n",
    "    # print values for b and w\n",
    "    b_t= b.eval()\n",
    "    w_t= w.eval()\n",
    "    #b_t= session.run(b)\n",
    "    print('b=', b_t)\n",
    "    print('w=', w_t)\n",
    "    \n",
    "    \n",
    "    # For positive network\n",
    "    print('a' + '-'*30 + 'pos' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        pos_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(pos_gen_test.y).eval({pos_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)\n",
    "    \n",
    "    # For negative network\n",
    "    print('a' + '-'*30 + 'neg' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        neg_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(neg_gen_test.y).eval({neg_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
