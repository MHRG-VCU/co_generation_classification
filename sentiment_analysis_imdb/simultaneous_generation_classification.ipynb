{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simultaneous generation-classification using LSTM \n",
    "Implementation of the paper: Simultaneous Generation-classification using lstm \n",
    "\n",
    "Authors: Daniel L. Marino, Kasun Amarasinghe, Milos Manic\n",
    "\n",
    "This script implements the training using Generation and Classification as objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#************************************************************************\n",
    "#      __   __  _    _  _____   _____\n",
    "#     /  | /  || |  | ||     \\ /  ___|\n",
    "#    /   |/   || |__| ||    _||  |  _\n",
    "#   / /|   /| ||  __  || |\\ \\ |  |_| |\n",
    "#  /_/ |_ / |_||_|  |_||_| \\_\\|______|\n",
    "#    \n",
    "#\n",
    "#   Copyright (2016) Modern Heuristics Research Group (MHRG)\n",
    "#   Virginia Commonwealth University (VCU), Richmond, VA\n",
    "#   http://www.people.vcu.edu/~mmanic/\n",
    "#   \n",
    "#   This program is free software: you can redistribute it and/or modify\n",
    "#   it under the terms of the GNU General Public License as published by\n",
    "#   the Free Software Foundation, either version 3 of the License, or\n",
    "#   (at your option) any later version.\n",
    "#\n",
    "#   This program is distributed in the hope that it will be useful,\n",
    "#   but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#   GNU General Public License for more details.\n",
    "#  \n",
    "#   Any opinions, findings, and conclusions or recommendations expressed \n",
    "#   in this material are those of the author's(s') and do not necessarily \n",
    "#   reflect the views of any other entity.\n",
    "#  \n",
    "#   ***********************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/marinodl/projects/co_generation_classification/sentiment_analysis_imdb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import range\n",
    "import time\n",
    "\n",
    "from twodlearn.tf_lib.Feedforward import LinearLayer\n",
    "from twodlearn.tf_lib.Recurrent import *\n",
    "\n",
    "import sys\n",
    "working_dir= os.getcwd()\n",
    "\n",
    "print('Working directory:', working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allow_gpu= True\n",
    "allow_valid= True\n",
    "allow_test= True\n",
    "Valid_Percentage= 0.1 # i.e. 10% of the data will be used for validation\n",
    "\n",
    "pos_net_name= '_pos'\n",
    "neg_net_name= '_neg'\n",
    "\n",
    "activation_function='tanh'\n",
    "num_nodes = [100, 100] #num_nodes: Nodes for the LSTM cell\n",
    "alpha = 10.0 #0.1 #0.1\n",
    "beta = 0.1 #0.01 #10000.01\n",
    "lambda_w = 0.00001\n",
    "\n",
    "dropout_cons = 0.8\n",
    "\n",
    "Allow_Bias= False \n",
    "\n",
    "learning_rate= 0.005      # 0.001\n",
    "grad_clip_thresh= 1.1       # 0.00001\n",
    "\n",
    "current_run= 1\n",
    "batch_size= 64 #64\n",
    "num_unrollings= 64 #100\n",
    "\n",
    "batch_size_val= 64 #len(valid_text_pos)/num_unrollings_val #500\n",
    "num_unrollings_val= num_unrollings #100\n",
    "\n",
    "batch_size_test= 64 \n",
    "num_unrollings_test= num_unrollings #100\n",
    "\n",
    "comment='_noDropout_LcLpLcp_64unrol_standarloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes: [100, 100]\n",
      "alpha: 10.0 , beta: 0.1 , lambda_w: 1e-05\n",
      "num_unrollings: 64 , batch_size: 64 , batch_size_val: 64\n",
      "learning_rate: 0.005 grad_clip_thresh: 1.1\n"
     ]
    }
   ],
   "source": [
    "model_version = 'L'+str(len(num_nodes))\n",
    "print(\"num_nodes:\",num_nodes)\n",
    "print(\"alpha:\",alpha, \", beta:\",beta,\", lambda_w:\", lambda_w)\n",
    "print(\"num_unrollings:\",num_unrollings, \", batch_size:\",batch_size,\", batch_size_val:\", batch_size_val)\n",
    "print(\"learning_rate:\", learning_rate, 'grad_clip_thresh:', grad_clip_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "(64, 1)\n",
      "(64, 4001)\n",
      "put togeth that you simpli have to some kind of fond for it half of the film at least revolv on the wacki adventur of and these two local own and run a boat rent shop near the lake but spend most of their day pick their nose and over fascin stuff like to spell the word it is mostli dure their laurel hardi situat \n",
      "[ 0.]\n",
      "factor that can tire the audienc due to lack of time to all of the action but it is in the end a detail homag to a great citi and suppli the viewer with mani to re visit like a of a time in pari it is a film worth see multipl time some movi seem to be made befor we are readi for them \n",
      "[ 1.]\n"
     ]
    }
   ],
   "source": [
    "vc= pickle.load( open( \"imdb_vc.pkl\", \"rb\" ) )\n",
    "num_inputs=  vc.vocabulary_size\n",
    "num_outputs= vc.vocabulary_size\n",
    "\n",
    "dataset= pickle.load( open( \"imdb_dataset.pkl\", \"rb\" ) )\n",
    "\n",
    "# set batch_size and number of unrolligns\n",
    "dataset.train.set_batch_and_unrollings(batch_size, num_unrollings)\n",
    "dataset.valid.set_batch_and_unrollings(batch_size_val, num_unrollings_val)\n",
    "dataset.test.set_batch_and_unrollings(batch_size_test, num_unrollings_test)\n",
    "\n",
    "# print a sample of the dataset\n",
    "train_x, train_y= dataset.train.next_batch()\n",
    "print(len(train_x))\n",
    "print(train_y.shape)\n",
    "print(train_x[0].shape)\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][0,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[0])\n",
    "\n",
    "print(vc.keys2text([np.argmax(train_x[i][50,:], 0) for i in range(len(train_x))]))\n",
    "print(train_y[50])\n",
    "\n",
    "valid_x, valid_y= dataset.valid.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# inputs: 4001\n",
      "pos train length: 2467784\n",
      "neg train length: 2467784\n",
      "pos valid length: 274198\n",
      "neg valid length: 274198\n",
      "pos test length: 2686382\n",
      "neg test length: 2686382\n"
     ]
    }
   ],
   "source": [
    "print('# inputs:',num_inputs)\n",
    "\n",
    "print('pos train length:',dataset.train.batch_generators[0]._text_size )\n",
    "print('neg train length:',dataset.train.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos valid length:',dataset.valid.batch_generators[0]._text_size )\n",
    "print('neg valid length:',dataset.valid.batch_generators[1]._text_size )\n",
    "\n",
    "print('pos test length:',dataset.test.batch_generators[0]._text_size )\n",
    "print('neg test length:',dataset.test.batch_generators[1]._text_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification_error(predictions, labels):\n",
    "    \"\"\" number of samples wrongly classified  \"\"\"\n",
    "    return np.sum( np.not_equal( np.greater(predictions, 0.5), \n",
    "                                 np.greater(labels, 0.5)) )/labels.shape[0]\n",
    "\n",
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10 # this is to prevent that log() returns minus infinity\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vc.vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vc.vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model for positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class myLstmNet(LstmNet):\n",
    "    def get_extra_inputs(self, i, h_list, state_list):\n",
    "        #print('OK:', len(h_list))\n",
    "        return i\n",
    "    \n",
    "    def evaluate_final_output(self, outputs_list, inputs_list, h_list ):\n",
    "        ''' Calculates the final output of the neural network, usually it is just a linear transformation\n",
    "            - outputs_list: list with the outputs from the last lstm cell\n",
    "            - inputs_list: list of inputs to the network\n",
    "            - h_list: list with all hidden outputs from all the cells\n",
    "        '''\n",
    "        all_hidden = list()\n",
    "        \n",
    "        for t in h_list: # go trough each time step\n",
    "            all_hidden.append( tf.concat(1,t) )\n",
    "        return self.out_layer.evaluate(tf.concat(0, all_hidden))  \n",
    "            \n",
    "    \n",
    "if len(num_nodes)>1:\n",
    "    n_extra= [num_inputs for i in range(len(num_nodes)+1)]\n",
    "    n_extra[0]= 0\n",
    "    n_extra[-1]= sum(num_nodes) - num_nodes[-1]\n",
    "else:\n",
    "    n_extra= [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelSetup:\n",
    "    \n",
    "    def __init__( self, pos_net, neg_net, w, b, batch_size, num_unrollings, drop_prob_list, name=''):\n",
    "    \n",
    "        # 1. Create placeholders for inputs \n",
    "        self.X = list()\n",
    "        for iaux in range(num_unrollings + 1):\n",
    "            self.X.append(tf.placeholder(tf.float32, shape=[batch_size, vc.vocabulary_size], \n",
    "                                             name= name+'X_i'+str(iaux)+'_All'))\n",
    "        aux_inputs = self.X[:num_unrollings]\n",
    "        aux_labels = self.X[1:]  # inputs shifted by one time step.\n",
    "\n",
    "        # Create a list for store the placeholders for the labels\n",
    "        self.labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "       \n",
    "\n",
    "        # -------------------- unrolling of the network --------------------------- # \n",
    "        self.pos_unroll, _= pos_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   )\n",
    "\n",
    "        self.neg_unroll, _= neg_net.unrolling_setup( batch_size, num_unrollings, \n",
    "                                                     inputs_list= aux_inputs,\n",
    "                                                     labels_list= aux_labels,\n",
    "                                                     drop_prob_list= drop_prob_list,\n",
    "                                                     reset_between_unrollings= True,\n",
    "                                                   ) \n",
    "\n",
    "\n",
    "        # Classifier.\n",
    "        # error_per_sample is a vector, its shape is changed to have each unrolling in separate columns\n",
    "        output_pos= tf.reshape(self.pos_unroll.error_per_sample,[num_unrollings,batch_size])  \n",
    "        output_neg= tf.reshape(self.neg_unroll.error_per_sample,[num_unrollings,batch_size]) \n",
    "\n",
    "        output_pos_mean= tf.reduce_mean(output_pos, reduction_indices= 0) \n",
    "        output_neg_mean= tf.reduce_mean(output_neg, reduction_indices= 0) \n",
    "\n",
    "        output_mean= tf.transpose(tf.pack( [ output_pos_mean, output_neg_mean ]))\n",
    "\n",
    "\n",
    "        self.logits = tf.nn.xw_plus_b( output_mean , w, b )\n",
    "\n",
    "\n",
    "        self.error_per_sample= tf.nn.sigmoid_cross_entropy_with_logits( self.logits, self.labels )\n",
    "        \n",
    "        # prediction error\n",
    "        #Lp = tf.reduce_mean( tf.mul(self.labels, output_pos_mean) + tf.mul(self.labels-1, output_neg_mean))\n",
    "        Lp = tf.reduce_mean( tf.mul(tf.squeeze(self.labels), output_pos_mean) + \n",
    "                             tf.mul(tf.squeeze(1-self.labels), output_neg_mean)\n",
    "                           )\n",
    "        # c-p penalty\n",
    "        Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.exp(-output_pos_mean)) + \n",
    "                              tf.mul(tf.squeeze(self.labels), tf.exp(-output_neg_mean))\n",
    "                            )\n",
    "        \n",
    "        #Lcp = tf.reduce_mean( tf.mul(tf.squeeze(1-self.labels), tf.reduce_mean(tf.exp(-output_pos), reduction_indices= 0) ) + \n",
    "        #                      tf.mul(tf.squeeze(self.labels), tf.reduce_mean(tf.exp(-output_neg), reduction_indices= 0) )\n",
    "        #                    )\n",
    "        \n",
    "        # regularization\n",
    "        l2_c= tf.nn.l2_loss(w)\n",
    "        \n",
    "        # loss\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp + beta*(1.0/Lcp)\n",
    "        #self.loss = tf.reduce_mean( self.error_per_sample ) + alpha*Lp - beta*(Lcp)\n",
    "        self.alpha_r = tf.placeholder(tf.float32)\n",
    "        self.beta_r = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.loss = tf.reduce_mean( self.error_per_sample ) + self.alpha_r*Lp + self.beta_r*Lcp + lambda_w*l2_c\n",
    "        \n",
    "        # classification error\n",
    "        self.error = tf.reduce_mean( tf.to_float(tf.not_equal( tf.greater(tf.nn.sigmoid(self.logits), 0.5), \n",
    "                                                               tf.greater(self.labels, 0.5))) )\n",
    "        \n",
    "        # perplexity measure\n",
    "        next_words_prob_pos = -tf.log( tf.nn.softmax( self.pos_unroll.y ) + 1e-10)\n",
    "        next_words_prob_neg = -tf.log( tf.nn.softmax( self.neg_unroll.y ) + 1e-10)\n",
    "            \n",
    "        next_words=  tf.concat(0,aux_labels) \n",
    "        \n",
    "        logprob_pos= tf.reshape( tf.reduce_sum( next_words_prob_pos * next_words, 1), [batch_size, num_unrollings] )\n",
    "        logprob_neg= tf.reshape( tf.reduce_sum( next_words_prob_neg * next_words, 1), [batch_size, num_unrollings] )  \n",
    "               \n",
    "        #self.perplexity_pos( -tf.log( prob_pos + 1e-10 ) * self.labels )\n",
    "        #self.perplexity_neg( -tf.log( prob_neg + 1e-10 ) * (1-self.labels) )\n",
    "        self.perplexity_pos = tf.reduce_sum( logprob_pos * self.labels )/(batch_size/2.0) \n",
    "        self.perplexity_neg = tf.reduce_sum( logprob_neg * (1-self.labels) )/(batch_size/2.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # For dropout\n",
    "    drop_prob = tf.placeholder(tf.float32)\n",
    "    drop_prob_list = [ drop_prob for i in range(len(num_nodes)+1)]\n",
    "    drop_prob_list[0]= None\n",
    "     \n",
    "    # 1. Define positive and negative neural networks\n",
    "    pos_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= pos_net_name)\n",
    "        \n",
    "    neg_net= myLstmNet( num_inputs, num_nodes, num_outputs, n_extra= n_extra,\n",
    "                        afunction=activation_function, \n",
    "                        LstmCell= AlexLstmCell,\n",
    "                        name= neg_net_name)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    #w = tf.Variable(tf.truncated_normal([2, 1], -0.1, 0.1), name=('w_class')) # unconstrained w\n",
    "    #w = tf.Variable(tf.constant( [[-2.0], [2.0]]), name=('w_class'), trainable=False) # fixed w\n",
    "    \n",
    "    #w = tf.Variable(tf.truncated_normal([1, 1], 0.3, 0.4), name=('w_class')) # unconstrained w\n",
    "    w = tf.Variable(tf.constant( [[0.3]] ), name=('w_class')) # fixed w\n",
    "    w = tf.concat(0, [-w, w])\n",
    "    \n",
    "    b = tf.Variable(tf.zeros([1]), name=('b_class'), trainable=Allow_Bias)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2. Define unrolling for training\n",
    "    train= ModelSetup( pos_net, neg_net, w, b, batch_size, num_unrollings, \n",
    "                       drop_prob_list, \n",
    "                       name='train_')\n",
    "       \n",
    "    # 3. Define unrolling for validation\n",
    "    if allow_valid:\n",
    "        valid= ModelSetup( pos_net, neg_net, w, b, batch_size_val, num_unrollings_val, \n",
    "                           drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                           name='valid_')\n",
    "        \n",
    "    # 4. Define unrolling for testing\n",
    "    if allow_test:\n",
    "        test= ModelSetup( pos_net, neg_net, w, b, batch_size_test, num_unrollings_test, \n",
    "                          drop_prob_list= [None for dummy in range(len(num_nodes))], \n",
    "                          name='test_')\n",
    "    \n",
    "    # 5. Define unrolling for testing generator\n",
    "    pos_gen_test, _= pos_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    neg_gen_test, _= neg_net.unrolling_setup( 1, 1, drop_prob_list= [None, None, None, None, None] )\n",
    "    \n",
    "          \n",
    "    \n",
    "    # 6. Define optimizer    \n",
    "    # 1. specify the optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate) # ADAM 0.001\n",
    "    \n",
    "    # 2. get the gradients and variables\n",
    "    # grads_and_vars is a list of tuples (gradient, variable). \n",
    "    grads_and_vars = optimizer.compute_gradients(train.loss) \n",
    "    gradients, v = zip(*grads_and_vars)\n",
    "    \n",
    "    # 3. process the gradients\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, grad_clip_thresh)  #1.25 #0.025 #0.001(last used)\n",
    "    # 4. apply the gradients to the optimization procedure\n",
    "    optimizer = optimizer.apply_gradients( zip(gradients, v) ) # ADAM\n",
    "    \n",
    "    # for prediction\n",
    "    train_pred = tf.nn.sigmoid(train.logits)\n",
    "    \n",
    "    # Saver\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train Energy predictor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reload_pos= False\n",
    "load_pos_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_pos.ckpt\"\n",
    "    \n",
    "reload_neg= False\n",
    "load_neg_file= working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                \"1r_neg.ckpt\"\n",
    "\n",
    "reload_all= True\n",
    "load_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run-1) +\"r_All\"+comment+\".ckpt\";\n",
    "\n",
    "save_all= False\n",
    "save_all_file=  working_dir+\"/weights/Weights_LSTM_\" + model_version +\"_\"+ str(num_nodes[0]) + \"u_\"+ \\\n",
    "                str(current_run) +\"r_All\"+comment+\".ckpt\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_run==1:\n",
    "    reload_all= False;\n",
    "else:\n",
    "    reload_pos= False;\n",
    "    reload_neg= False;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights Initialized\n",
      "step | Train_err | Valid_err | loss\n",
      "0 | 0.453125 | 0.490938 | 83.768066 | 24.296359\n",
      "50 | 0.358125 | 0.287500 | 67.880219 | 73.715516\n",
      "100 | 0.279375 | 0.290938 | 62.680808 | 122.749064\n",
      "150 | 0.325313 | 0.298125 | 59.963949 | 171.586607\n",
      "200 | 0.262500 | 0.246562 | 56.997368 | 221.150262\n",
      "250 | 0.248750 | 0.235625 | 55.792419 | 271.531565\n",
      "300 | 0.224375 | 0.235937 | 55.243830 | 321.085990\n",
      "350 | 0.229063 | 0.224688 | 54.222430 | 370.170664\n",
      "400 | 0.225000 | 0.208750 | 54.029170 | 419.081002\n",
      "450 | 0.204063 | 0.200625 | 53.445030 | 468.074180\n",
      "500 | 0.218125 | 0.207187 | 53.156141 | 517.116755\n",
      "550 | 0.206563 | 0.209687 | 52.608243 | 565.985363\n",
      "600 | 0.205937 | 0.209375 | 52.570417 | 614.698818\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "650 | 0.225000 | 0.195937 | 52.529559 | 663.276313\n",
      "700 | 0.213438 | 0.211562 | 52.103908 | 711.893532\n",
      "750 | 0.184375 | 0.203125 | 51.865010 | 760.550121\n",
      "800 | 0.205000 | 0.201250 | 51.730569 | 808.946517\n",
      "850 | 0.218438 | 0.205000 | 51.817665 | 857.482770\n",
      "900 | 0.186875 | 0.193750 | 51.453548 | 905.987821\n",
      "950 | 0.182188 | 0.192812 | 51.244516 | 954.485095\n",
      "1000 | 0.194688 | 0.201563 | 51.449783 | 1002.954417\n",
      "1050 | 0.212500 | 0.183438 | 50.996724 | 1051.429606\n",
      "1100 | 0.191562 | 0.192500 | 51.229385 | 1099.989227\n",
      "1150 | 0.207187 | 0.197187 | 50.893779 | 1148.504692\n",
      "1200 | 0.184688 | 0.190625 | 50.830305 | 1196.879843\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1250 | 0.156875 | 0.188750 | 50.603657 | 1245.352570\n",
      "1300 | 0.170000 | 0.183125 | 50.698833 | 1293.876009\n",
      "1350 | 0.147500 | 0.185938 | 50.483524 | 1342.274682\n",
      "1400 | 0.145313 | 0.187500 | 49.976222 | 1390.707846\n",
      "1450 | 0.142187 | 0.177187 | 50.006600 | 1439.430996\n",
      "1500 | 0.130000 | 0.187500 | 49.987047 | 1487.949126\n",
      "1550 | 0.124375 | 0.188750 | 49.479780 | 1536.503792\n",
      "1600 | 0.127812 | 0.180312 | 49.691076 | 1584.925099\n",
      "1650 | 0.099687 | 0.185938 | 49.302946 | 1633.498947\n",
      "1700 | 0.108438 | 0.183438 | 49.330178 | 1681.869570\n",
      "1750 | 0.110312 | 0.187188 | 48.932823 | 1730.267089\n",
      "1800 | 0.097500 | 0.182812 | 49.059352 | 1778.758862\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "1850 | 0.105938 | 0.188125 | 49.152913 | 1827.294442\n",
      "1900 | 0.095312 | 0.202813 | 48.952640 | 1875.704167\n",
      "1950 | 0.075313 | 0.183438 | 48.740326 | 1924.037077\n",
      "2000 | 0.079062 | 0.182188 | 48.643442 | 1972.547229\n",
      "2050 | 0.087188 | 0.199375 | 48.792571 | 2021.011301\n",
      "2100 | 0.068437 | 0.185625 | 48.647409 | 2069.401333\n",
      "2150 | 0.063125 | 0.180312 | 48.428013 | 2117.675226\n",
      "2200 | 0.074063 | 0.190000 | 48.741878 | 2166.068000\n",
      "2250 | 0.074063 | 0.179688 | 48.398177 | 2214.561707\n",
      "2300 | 0.065000 | 0.193750 | 48.593586 | 2262.901230\n",
      "2350 | 0.067812 | 0.193750 | 48.333766 | 2311.240198\n",
      "2400 | 0.056563 | 0.182500 | 48.410298 | 2359.530586\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "2450 | 0.044687 | 0.195625 | 48.274362 | 2407.832983\n",
      "2500 | 0.050625 | 0.179063 | 48.347916 | 2456.269449\n",
      "2550 | 0.039688 | 0.196250 | 48.266091 | 2504.625879\n",
      "2600 | 0.042188 | 0.189375 | 47.848904 | 2552.960023\n",
      "2650 | 0.037812 | 0.171563 | 47.869737 | 2601.340207\n",
      "2700 | 0.037187 | 0.196875 | 47.955788 | 2649.708343\n",
      "2750 | 0.029687 | 0.193125 | 47.605071 | 2698.097428\n",
      "2800 | 0.028125 | 0.184688 | 47.592421 | 2746.414474\n",
      "2850 | 0.023125 | 0.192500 | 47.475824 | 2794.834029\n",
      "2900 | 0.022812 | 0.194688 | 47.509633 | 2843.307361\n",
      "2950 | 0.027813 | 0.178750 | 47.183422 | 2891.652028\n",
      "3000 | 0.020000 | 0.197500 | 47.266836 | 2940.075033\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "\n",
      "3050 | 0.021875 | 0.184062 | 47.356322 | 2988.579567\n",
      "3100 | 0.020313 | 0.197187 | 47.301341 | 3037.206577\n",
      "3150 | 0.017188 | 0.185625 | 47.107063 | 3085.594855\n",
      "3200 | 0.012500 | 0.183750 | 46.963841 | 3134.395439\n",
      "3250 | 0.013750 | 0.195312 | 47.119653 | 3182.960052\n",
      "3300 | 0.014063 | 0.195000 | 47.090900 | 3231.483391\n",
      "3350 | 0.011875 | 0.190000 | 46.786218 | 3283.509098\n",
      "3400 | 0.011562 | 0.194375 | 47.220419 | 3332.826921\n",
      "3450 | 0.013125 | 0.194375 | 46.858940 | 3382.155156\n",
      "3500 | 0.010000 | 0.201250 | 47.037580 | 3431.210775\n"
     ]
    }
   ],
   "source": [
    "num_steps = 6000 \n",
    "summary_frequency = 50\n",
    "n_valid_tests = 50\n",
    "n_characters_step= num_unrollings*batch_size\n",
    "\n",
    "train_error_l= list()\n",
    "valid_error_l= list()\n",
    "\n",
    "if allow_gpu:\n",
    "    config= None\n",
    "else:\n",
    "    config = tf.ConfigProto( device_count = {'GPU': 0} )\n",
    "\n",
    "aux_print=0\n",
    "with tf.Session(graph=graph, config=config) as session:\n",
    "    # -------------------------------- Load weigths from file, or initialize variables ------------------------------------\n",
    "    if reload_all:\n",
    "        saver.restore(session, load_all_file)\n",
    "        #session.run( global_step.assign(0) ) # SGD\n",
    "        \n",
    "    elif (reload_pos and reload_neg):\n",
    "        tf.initialize_all_variables().run()\n",
    "        pos_net.saver.restore(session, load_pos_file)\n",
    "        neg_net.saver.restore(session, load_neg_file)\n",
    "        print('Weights for positive and negative networks loaded')\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print('Weights Initialized')\n",
    "    \n",
    "    # ------------------------------------------- Training loop --------------------------------------------------\n",
    "    print('step | Train_err | Valid_err | loss')\n",
    "    \n",
    "    mean_loss = 0.0\n",
    "    mean_error = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # 1. Get next batch\n",
    "        batch_X, batch_y = dataset.train.next_batch()\n",
    "        \n",
    "        # 2. Setup feed dictionary for network 1 and network 2\n",
    "        feed_dict = dict()\n",
    "        # For dropout\n",
    "        feed_dict[drop_prob] = dropout_cons\n",
    "        # hyperparameters\n",
    "        feed_dict[train.alpha_r] = alpha\n",
    "        feed_dict[train.beta_r] = beta\n",
    "            \n",
    "        # Inroduce labels\n",
    "        feed_dict[train.labels] = batch_y\n",
    "        # Introduce inputs\n",
    "        for i in range(num_unrollings+1):\n",
    "            feed_dict[train.X[i]] = batch_X[i]\n",
    "              \n",
    "        # 3 Run optimizer\n",
    "        #_, l, lr = session.run( [optimizer, loss_train, learning_rate], feed_dict=feed_dict) # SGD\n",
    "        _, l, train_error = session.run( [optimizer, train.loss, train.error], feed_dict=feed_dict) # ADAM\n",
    "        \n",
    "        mean_loss += l\n",
    "        mean_error += train_error #classification_error(pred_train_aux, batch_y) \n",
    "        \n",
    "        # --------------------------------------------- logging ------------------------------------------------\n",
    "        if dataset.train.batch_generators[0]._text_size<aux_print :\n",
    "            print('+'*80)\n",
    "            print('')\n",
    "            aux_print = 0\n",
    "        else:\n",
    "            aux_print += n_characters_step\n",
    "        \n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                mean_error = mean_error / summary_frequency\n",
    "                        \n",
    "            # ---------- Print loss in validation dataset ---------\n",
    "            if allow_valid:\n",
    "                ''' classification error on validation dataset '''\n",
    "                mean_error_val = 0.0\n",
    "                for step_valid in range(n_valid_tests):\n",
    "                    # 1. Get next batch\n",
    "                    batch_X_val, batch_y_val = dataset.valid.next_batch()        \n",
    "                    feed_dict_val = dict()\n",
    "                    # For dropout\n",
    "                    feed_dict_val[drop_prob] = 1.0\n",
    "                    # Inroduce labels\n",
    "                    feed_dict_val[valid.labels] = batch_y_val\n",
    "                    # Introduce inputs\n",
    "                    for i in range(num_unrollings_val+1):\n",
    "                        feed_dict_val[valid.X[i]] = batch_X_val[i]\n",
    "                    # 2. Get classification error\n",
    "                    [valid_error] = session.run( [valid.error], feed_dict=feed_dict_val)          \n",
    "                    mean_error_val += valid_error\n",
    "                    \n",
    "                mean_error_val = mean_error_val/n_valid_tests\n",
    "                \n",
    "                ''' print information '''\n",
    "                train_error_l.append(mean_error)\n",
    "                valid_error_l.append(mean_error_val)\n",
    "                \n",
    "                if save_all and valid_error_l[-1] == min(valid_error_l) and step>200:\n",
    "                    save_path = saver.save(session, save_all_file)\n",
    "                    print('%d | %f | %f | %f | saved' % (step, \n",
    "                                                         mean_error, \n",
    "                                                         mean_error_val, \n",
    "                                                         mean_loss\n",
    "                                                        ))    \n",
    "                \n",
    "                else:\n",
    "                    cur_time = time.time() - start_time\n",
    "                    print('%d | %f | %f | %f | %f' % (step, \n",
    "                                                      mean_error, \n",
    "                                                      mean_error_val, \n",
    "                                                      mean_loss,\n",
    "                                                      cur_time\n",
    "                                                 ))   \n",
    "                    \n",
    "                #if(mean_error_val < 0.19):\n",
    "                #    break\n",
    "            else:\n",
    "                train_error_l.append(mean_error)\n",
    "                print('%d | %f | %f' % (step, mean_error, mean_loss))\n",
    "                                       \n",
    "            mean_loss = 0\n",
    "            mean_error = 0\n",
    "            \n",
    "    if save_all and allow_valid:\n",
    "        print(\"Learning finished, weights saved in file: %s\" % save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "train_error_a = np.array(train_error_l)\n",
    "valid_error_a = np.array(valid_error_l)\n",
    "\n",
    "plt.plot(np.linspace(0, train_error_a.shape[0], train_error_a.shape[0]) , train_error_a )\n",
    "plt.plot(np.linspace(0, valid_error_a.shape[0], train_error_a.shape[0]) , valid_error_a )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification error on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_tests = 600\n",
    "if allow_test:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # Load weigths from file, or initialize variables\n",
    "        tf.initialize_all_variables().run()\n",
    "        saver.restore(session, save_all_file)\n",
    "        print('Weights loaded')\n",
    "        \n",
    "        # calculate errors using mini-batches\n",
    "        mean_error_test = 0\n",
    "        mean_perp_pos = 0\n",
    "        mean_perp_neg = 0\n",
    "        for step_test in range(n_tests):\n",
    "            # 1. Get next batch\n",
    "            batch_X_test, batch_y_test = dataset.test.next_batch()        \n",
    "            feed_dict_test = dict()\n",
    "            # For dropout\n",
    "            feed_dict_test[drop_prob] = 1.0\n",
    "            # Inroduce labels\n",
    "            feed_dict_test[test.labels] = batch_y_test\n",
    "            # Introduce inputs\n",
    "            for i in range(num_unrollings_test+1):\n",
    "                feed_dict_test[test.X[i]] = batch_X_test[i]\n",
    "            # 2. Get prediction\n",
    "            [test_error, perp_pos, perp_neg] = session.run( [test.error, test.perplexity_pos, test.perplexity_neg], \n",
    "                                                            feed_dict=feed_dict_test)          \n",
    "            mean_error_test += test_error\n",
    "            mean_perp_pos += perp_pos\n",
    "            mean_perp_neg += perp_neg\n",
    "            \n",
    "        print(\"Classification error on test dataset:\", mean_error_test)\n",
    "        print(\"Perplexity on positive dataset:\", mean_perp_pos)\n",
    "        print(\"Perplexity on negative dataset:\", mean_perp_neg)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some samples from positive and negative networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # Load weigths from file, or initialize variables\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver.restore(session, save_all_file)\n",
    "    print('Weights loaded')\n",
    "        \n",
    "    # print values for b and w\n",
    "    b_t= b.eval()\n",
    "    w_t= w.eval()\n",
    "    #b_t= session.run(b)\n",
    "    print('b=', b_t)\n",
    "    print('w=', w_t)\n",
    "    \n",
    "    \n",
    "    # For positive network\n",
    "    print('a' + '-'*30 + 'pos' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        pos_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(pos_gen_test.y).eval({pos_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)\n",
    "    \n",
    "    # For negative network\n",
    "    print('a' + '-'*30 + 'neg' + '-'*30)\n",
    "    for _ in range(5):\n",
    "        feed = sample(random_distribution())\n",
    "        sentence = vc.prob2char(feed)\n",
    "        neg_gen_test.reset_saved_out_state.run()\n",
    "        for _ in range(30):\n",
    "            prediction = tf.nn.softmax(neg_gen_test.y).eval({neg_gen_test.inputs_list[0]: feed}) # feed is a 1-hot encoding\n",
    "            feed = sample(prediction)  # sample returns a 1-hot encoding\n",
    "            sentence += vc.prob2char(feed)\n",
    "        print(sentence)\n",
    "    print('a' + '-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
